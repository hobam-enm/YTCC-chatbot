# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„ê¸° â€” ì±—ë´‡ ëª¨ë“œ (ë¯¸ë‹ˆë©€ UI)
# - ìì—°ì–´ í•œ ì¤„ â†’ (ê¸°ê°„/í‚¤ì›Œë“œ/ì˜µì…˜) í•´ì„(Gemini) â†’ ìˆ˜ì§‘ â†’ ìš”ì•½/ì‹œê°í™”
# - UIëŠ” ê²€ìƒ‰ë°” 1ê°œ + ê²°ê³¼ ìƒë‹¨ì— ì‘ì€ ë°°ì§€(ë¶„ì„í‚¤ì›Œë“œ/ë¶„ì„ê¸°ê°„) + ì‹¬í”Œ ë¡œë”©
# - ë‹¤ìš´ë¡œë“œ: ì „ì²´ëŒ“ê¸€ CSV, ì „ì²´ì˜ìƒëª©ë¡ CSV

import streamlit as st
import pandas as pd
import os, re, json, gc, time
from datetime import datetime, timedelta, timezone
from uuid import uuid4
from concurrent.futures import ThreadPoolExecutor, as_completed

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
import plotly.express as px

# ===================== ê¸°ë³¸ ì„¤ì • =====================
st.set_page_config(page_title="ğŸ’¬ ëŒ“ê¸€ë¶„ì„ê¸°: ì±—ë´‡ ëª¨ë“œ", layout="wide", initial_sidebar_state="centered")

# í—¤ë”(ìŠ¤í”Œë˜ì‹œ ëŠë‚Œ)
st.markdown(
    """
    <div style="text-align:center; margin-top:24px; margin-bottom:12px">
      <div style="font-size:38px; font-weight:700; letter-spacing:-0.5px;">Streamlit AI assistant</div>
      <div style="color:#6b7280; margin-top:6px">Ask a questionâ€¦</div>
    </div>
    """,
    unsafe_allow_html=True,
)

# ===================== ë¹„ë°€í‚¤/ìƒìˆ˜ =====================
BASE_DIR = "/tmp"; os.makedirs(BASE_DIR, exist_ok=True)
KST = timezone(timedelta(hours=9))

_YT_FALLBACK = []
_GEM_FALLBACK = []
YT_API_KEYS = list(st.secrets.get("YT_API_KEYS", [])) or _YT_FALLBACK
GEMINI_API_KEYS = list(st.secrets.get("GEMINI_API_KEYS", [])) or _GEM_FALLBACK
GEMINI_MODEL = st.secrets.get("GEMINI_MODEL", "gemini-2.0-flash-lite")
GEMINI_TIMEOUT = int(st.secrets.get("GEMINI_TIMEOUT", 120))
GEMINI_MAX_TOKENS = int(st.secrets.get("GEMINI_MAX_TOKENS", 2048))

MAX_TOTAL_COMMENTS = 120_000
MAX_COMMENTS_PER_VIDEO = 4_000

# ===================== ìœ í‹¸ =====================

def now_kst():
    return datetime.now(tz=KST)

def to_iso_kst(dt: datetime) -> str:
    if dt.tzinfo is None: dt = dt.replace(tzinfo=KST)
    return dt.astimezone(KST).isoformat(timespec="seconds")

def kst_to_rfc3339_utc(dt_kst: datetime) -> str:
    if dt_kst.tzinfo is None: dt_kst = dt_kst.replace(tzinfo=KST)
    return dt_kst.astimezone(timezone.utc).isoformat().replace("+00:00","Z")

class RotatingKeys:
    def __init__(self, keys, state_key: str):
        ks = [k.strip() for k in (keys or []) if isinstance(k, str) and k.strip()]
        self.keys = ks[:10]; self.state_key = state_key
        self.idx = st.session_state.get(state_key, 0) % max(1, len(self.keys))
        st.session_state[state_key] = self.idx
    def current(self):
        return self.keys[self.idx] if self.keys else None
    def rotate(self):
        if not self.keys: return
        self.idx = (self.idx + 1) % len(self.keys)
        st.session_state[self.state_key] = self.idx

class RotatingYouTube:
    def __init__(self, keys):
        self.rot = RotatingKeys(keys, "yt_key_idx")
        key = self.rot.current();
        if not key: raise RuntimeError("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
        self.svc = build("youtube", "v3", developerKey=key)
    def execute(self, factory):
        try:
            return factory(self.svc).execute()
        except HttpError as e:
            status = getattr(getattr(e, 'resp', None), 'status', None)
            msg = (getattr(e, 'content', b'').decode('utf-8', errors='ignore') or '').lower()
            if status in (403,429) and any(t in msg for t in ["quota","rate","limit"]) and len(YT_API_KEYS) > 1:
                self.rot.rotate(); self.svc = build("youtube", "v3", developerKey=self.rot.current())
                return factory(self.svc).execute()
            raise

# ===================== Gemini í˜¸ì¶œ & í”„ë¡¬í”„íŠ¸ =====================
LIGHT_PROMPT = (
    "ì—­í• : ë‹¹ì‹ ì€ â€˜ìœ íŠœë¸Œ ëŒ“ê¸€ ë°˜ì‘ ë¶„ì„ê¸°â€™ë¥¼ ìœ„í•œ ìì—°ì–´ í•´ì„ê°€ë‹¤.\n"
    "ëª©í‘œ: ì‚¬ìš©ìì˜ ìš”ì²­ì—ì„œ [ê²€ìƒ‰ ê¸°ê°„]ê³¼ [ê²€ìƒ‰ í‚¤ì›Œë“œ(ì£¼ì œ/ì—”í‹°í‹°/ë³´ì¡°ì–´)]ë¥¼ í•´ì„í•œë‹¤.\n"
    "ì›ì¹™: ê¸°ê°„ì€ KST(+09:00)ë¡œ, ìƒëŒ€ê¸°ê°„ì˜ ì¢…ë£Œì‹œì ì€ ì§€ê¸ˆ. ì˜µì…˜(ëŒ€ëŒ“ê¸€ í¬í•¨/ê³µì‹ì±„ë„/ì–¸ì–´)ë„ ê°ì§€.\n\n"
    "ì¶œë ¥ í˜•ì‹(6ì¤„ ê³ ì •):\n"
    "- í•œ ì¤„ ìš”ì•½: <ë¬¸ì¥>\n"
    "- ê¸°ê°„(KST): <YYYY-MM-DDTHH:MM:SS+09:00> ~ <YYYY-MM-DDTHH:MM:SS+09:00>\n"
    "- í‚¤ì›Œë“œ: [<ë©”ì¸1>, <ë©”ì¸2>â€¦]\n"
    "- ì—”í‹°í‹°/ë³´ì¡°: [<ë³´ì¡°ë“¤>]\n"
    "- ì˜µì…˜: { include_replies: true|false, channel_filter: \"any|official|unofficial\", lang: \"ko|en|auto\" }\n"
    "- ì›ë¬¸: {USER_QUERY}\n\n"
    f"ì§€ê¸ˆ ì‹œê°„ KST: {to_iso_kst(now_kst())}\nì•„ë˜ ì…ë ¥ í•´ì„:\n\n{{USER_QUERY}}"
)

def call_gemini(prompt: str) -> str:
    rk = RotatingKeys(GEMINI_API_KEYS, "gem_key_idx")
    if not rk.current(): raise RuntimeError("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=rk.current())
    model = genai.GenerativeModel(GEMINI_MODEL, generation_config={"temperature":0.2,"max_output_tokens":GEMINI_MAX_TOKENS})
    try:
        resp = model.generate_content([prompt], request_options={"timeout": GEMINI_TIMEOUT})
        return getattr(resp, "text", "") or ""
    except Exception:
        if len(GEMINI_API_KEYS) > 1:
            rk.rotate(); genai.configure(api_key=rk.current())
            resp = model.generate_content([prompt], request_options={"timeout": GEMINI_TIMEOUT})
            return getattr(resp, "text", "") or ""
        raise

# 6ì¤„ ë¼ì´íŠ¸ ë¸”ë¡ â†’ í‘œì¤€ ìŠ¤í‚¤ë§ˆ

def parse_light_block_to_schema(light_text: str) -> dict:
    raw = (light_text or "").strip()
    m_time = re.search(r"ê¸°ê°„\(KST\)\s*:\s*([^~]+)~\s*([^\n]+)", raw)
    start_iso = m_time.group(1).strip() if m_time else None
    end_iso   = m_time.group(2).strip() if m_time else None
    m_kw = re.search(r"í‚¤ì›Œë“œ\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    keywords = []
    if m_kw:
        for part in re.split(r"\s*,\s*", m_kw.group(1)):
            p = re.sub(r"\(.*?\)", "", part).strip()
            if p: keywords.append(p)
    m_ent = re.search(r"ì—”í‹°í‹°/ë³´ì¡°\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    entities = []
    if m_ent:
        for part in re.split(r"\s*,\s*", m_ent.group(1)):
            p = part.strip()
            if p: entities.append(p)
    m_opt = re.search(r"ì˜µì…˜\s*:\s*\{(.*?)\}", raw, flags=re.DOTALL)
    options = {"include_replies": False, "channel_filter": "any", "lang": "auto"}
    if m_opt:
        blob = m_opt.group(1)
        ir = re.search(r"include_replies\s*:\s*(true|false)", blob, re.I)
        cf = re.search(r"channel_filter\s*:\s*\"(any|official|unofficial)\"", blob, re.I)
        lg = re.search(r"lang\s*:\s*\"(ko|en|auto)\"", blob, re.I)
        if ir: options["include_replies"] = (ir.group(1).lower()=="true")
        if cf: options["channel_filter"] = cf.group(1)
        if lg: options["lang"] = lg.group(1)
    if not start_iso or not end_iso:
        end_dt = now_kst(); start_dt = end_dt - timedelta(hours=24)
        start_iso, end_iso = to_iso_kst(start_dt), to_iso_kst(end_dt)
    if not keywords:
        m = re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", raw)
        keywords = [m[0]] if m else ["ìœ íŠœë¸Œ"]
    return {"start_iso": start_iso, "end_iso": end_iso, "keywords": keywords, "entities": entities, "options": options, "raw": raw}

# ===================== YouTube ê²€ìƒ‰/ìˆ˜ì§‘ =====================

def yt_search_videos(rt, keyword, max_results, order="relevance", published_after=None, published_before=None):
    vids, token = [], None
    while len(vids) < max_results:
        params = dict(q=keyword, part="id", type="video", order=order, maxResults=min(50, max_results - len(vids)))
        if published_after: params["publishedAfter"] = published_after
        if published_before: params["publishedBefore"] = published_before
        if token: params["pageToken"] = token
        resp = rt.execute(lambda s: s.search().list(**params))
        for it in resp.get("items", []):
            vid = it["id"]["videoId"]
            if vid not in vids: vids.append(vid)
        token = resp.get("nextPageToken")
        if not token: break
        time.sleep(0.25)
    return vids

def yt_video_statistics(rt, video_ids):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        if not batch: continue
        resp = rt.execute(lambda s: s.videos().list(part="statistics,snippet,contentDetails", id=",".join(batch)))
        for item in resp.get("items", []):
            stats = item.get("statistics", {}); snip = item.get("snippet", {})
            rows.append({
                "video_id": item.get("id"),
                "video_url": f"https://www.youtube.com/watch?v={item.get('id')}",
                "title": snip.get("title",""),
                "channelTitle": snip.get("channelTitle",""),
                "publishedAt": snip.get("publishedAt",""),
                "viewCount": int(stats.get("viewCount",0) or 0),
                "likeCount": int(stats.get("likeCount",0) or 0),
                "commentCount": int(stats.get("commentCount",0) or 0),
            })
        time.sleep(0.25)
    return rows

from googleapiclient.errors import HttpError

def yt_all_replies(rt, parent_id, video_id, title=""):
    replies, token = [], None
    while True:
        params = dict(part="snippet", parentId=parent_id, maxResults=100, pageToken=token, textFormat="plainText")
        try:
            resp = rt.execute(lambda s: s.comments().list(**params))
        except HttpError:
            break
        for c in resp.get("items", []):
            sn = c["snippet"]
            replies.append({
                "video_id": video_id, "video_title": title, "isReply": 1,
                "comment_id": c.get("id",""), "parent_id": parent_id,
                "author": sn.get("authorDisplayName",""),
                "text": sn.get("textDisplay","") or "",
                "publishedAt": sn.get("publishedAt",""),
                "likeCount": int(sn.get("likeCount",0) or 0),
            })
        token = resp.get("nextPageToken")
        if not token: break
        time.sleep(0.2)
    return replies


def yt_all_comments_sync(rt, video_id, title="", include_replies=True, max_per_video=None):
    rows, token = [], None
    while True:
        if max_per_video is not None and len(rows) >= max_per_video:
            return rows[:max_per_video]
        params = dict(part="snippet,replies", videoId=video_id, maxResults=100, pageToken=token, textFormat="plainText")
        try:
            resp = rt.execute(lambda s: s.commentThreads().list(**params))
        except HttpError:
            break
        for it in resp.get("items", []):
            top = it["snippet"]["topLevelComment"]["snippet"]
            thread_id = it["snippet"]["topLevelComment"]["id"]
            total_replies = int(it["snippet"].get("totalReplyCount",0) or 0)
            rows.append({
                "video_id": video_id, "video_title": title, "isReply": 0,
                "comment_id": thread_id, "parent_id": "",
                "author": top.get("authorDisplayName",""),
                "text": top.get("textDisplay","") or "",
                "publishedAt": top.get("publishedAt",""),
                "likeCount": int(top.get("likeCount",0) or 0),
            })
            if include_replies and total_replies > 0:
                cap = None if max_per_video is None else max(0, max_per_video - len(rows))
                if cap == 0: return rows[:max_per_video]
                rows.extend(yt_all_replies(rt, thread_id, video_id, title)[:cap if cap else None])
                if max_per_video is not None and len(rows) >= max_per_video:
                    return rows[:max_per_video]
        token = resp.get("nextPageToken")
        if not token: break
        time.sleep(0.2)
    return rows


def parallel_collect_comments_streaming(video_list, rt_keys, include_replies, max_total_comments, max_per_video, log=None, prog=None):
    out_csv = os.path.join(BASE_DIR, f"collect_{uuid4().hex}.csv")
    wrote_header = False; total_written = 0
    total_videos = len(video_list); done = 0
    with ThreadPoolExecutor(max_workers=3) as ex:
        futures = {ex.submit(yt_all_comments_sync, RotatingYouTube(rt_keys), v["video_id"], v.get("title",""), include_replies, max_per_video): v for v in video_list}
        for f in as_completed(futures):
            v = futures[f]
            try:
                comm = f.result()
                if comm:
                    dfc = pd.DataFrame(comm)
                    dfc.to_csv(out_csv, index=False, mode=("a" if wrote_header else "w"), header=(not wrote_header), encoding="utf-8-sig")
                    wrote_header = True; total_written += len(dfc)
            except Exception as e:
                if log: log(f"ì‹¤íŒ¨: {v.get('title','')} â€” {e}")
            done += 1
            if log: log(f"ì§„í–‰ {done}/{total_videos}")
            if prog: prog(done/total_videos)
            if total_written >= max_total_comments:
                break
    return out_csv, total_written

# LLM ì§ë ¬í™”(ê°„ë‹¨)

def serialize_comments_for_llm_from_file(csv_path: str, max_rows=1500, max_chars_per_comment=280, max_total_chars=420_000):
    if not csv_path or not os.path.exists(csv_path): return "", 0, 0
    lines, total = [], 0; remaining = max_rows
    for chunk in pd.read_csv(csv_path, chunksize=120_000):
        if "likeCount" in chunk.columns:
            chunk = chunk.sort_values("likeCount", ascending=False)
        for _, r in chunk.iterrows():
            if remaining <= 0 or total >= max_total_chars: break
            is_reply = "R" if int(r.get("isReply",0) or 0)==1 else "T"
            author = str(r.get("author","") or "").replace("\n"," ")
            likec = int(r.get("likeCount",0) or 0)
            text = str(r.get("text","") or "").replace("\n"," ")
            if len(text) > max_chars_per_comment: text = text[:max_chars_per_comment] + "â€¦"
            line = f"[{is_reply}|â™¥{likec}] {author}: {text}"
            if total + len(line) + 1 > max_total_chars: break
            lines.append(line); total += len(line)+1; remaining -= 1
        if remaining <= 0 or total >= max_total_chars: break
    return "\n".join(lines), len(lines), total

# ===================== ê²€ìƒ‰ ì…ë ¥(í•œì¤„) =====================
with st.container():
    c1, c2, c3 = st.columns([1,6,1])
    with c2:
        q = st.text_input(" ", placeholder="ìµœê·¼ 12ì‹œê°„ íƒœí’ìƒì‚¬ ê¹€ì¤€í˜¸ ëŒ“ê¸€ë°˜ì‘ ë¶„ì„í•´ì¤˜", label_visibility="collapsed", key="query")
    run = st.button("â–¶", help="ì‹¤í–‰", use_container_width=False)

# ===================== ì‹¤í–‰ =====================
if run and q:
    if not (GEMINI_API_KEYS and YT_API_KEYS):
        st.error("API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤ (YT_API_KEYS / GEMINI_API_KEYS).")
    else:
        # 1) í•´ì„
        with st.status("ìš”ì²­ í•´ì„ ì¤‘â€¦", expanded=False) as s:
            prompt = LIGHT_PROMPT.replace("{USER_QUERY}", q)
            light = call_gemini(prompt)
            schema = parse_light_block_to_schema(light)
            s.update(label="í•´ì„ ì™„ë£Œ", state="complete")

        # 2) íŒŒë¼ë¯¸í„° ì¤€ë¹„
        start_dt = datetime.fromisoformat(schema["start_iso"]).astimezone(KST)
        end_dt   = datetime.fromisoformat(schema["end_iso"]).astimezone(KST)
        published_after = kst_to_rfc3339_utc(start_dt)
        published_before = kst_to_rfc3339_utc(end_dt)
        keywords = schema.get("keywords", [])
        entities = schema.get("entities", [])
        include_replies = bool(schema.get("options",{}).get("include_replies", False))

        # 3) ìˆ˜ì§‘
        with st.status("ì˜ìƒ/ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘â€¦", expanded=True) as s:
            rt = RotatingYouTube(YT_API_KEYS)
            ids_all = []
            for base in (keywords or ["ìœ íŠœë¸Œ"]):
                ids_all += yt_search_videos(rt, base, 60, "relevance", published_after, published_before)
                for e in (entities or []):
                    ids_all += yt_search_videos(rt, f"{base} {e}", 30, "relevance", published_after, published_before)
            ids_all = list(dict.fromkeys(ids_all))
            s.write(f"ğŸï¸ ëŒ€ìƒ ì˜ìƒ: {len(ids_all)}")
            stats = yt_video_statistics(rt, ids_all)
            df_stats = pd.DataFrame(stats)
            if not df_stats.empty and "publishedAt" in df_stats.columns:
                df_stats["publishedAt_kst"] = pd.to_datetime(df_stats["publishedAt"], errors="coerce", utc=True).dt.tz_convert("Asia/Seoul").dt.strftime("%Y-%m-%d %H:%M:%S")
            prog = st.progress(0)
            csv_path, total_cnt = parallel_collect_comments_streaming(df_stats.to_dict('records'), YT_API_KEYS, include_replies, MAX_TOTAL_COMMENTS, MAX_COMMENTS_PER_VIDEO, log=s.write, prog=prog.progress)
            s.write(f"ì´ ëŒ“ê¸€ ìˆ˜ì§‘: {total_cnt:,}ê°œ")
            s.update(label="ìˆ˜ì§‘ ì™„ë£Œ", state="complete")

        if total_cnt == 0:
            st.warning("ëŒ“ê¸€ì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ê°„/í‚¤ì›Œë“œë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")
            st.stop()

        # 4) ìƒë‹¨ ë©”íƒ€ ë°°ì§€
        kw_badge = ", ".join(keywords) if keywords else "(ì—†ìŒ)"
        period_badge = f"{schema['start_iso']} ~ {schema['end_iso']}"
        st.markdown(
            f"<div style='margin:8px 0 4px 0'>"
            f"<span style='background:#f3f4f6; padding:6px 10px; border-radius:999px; margin-right:6px; font-size:12px;'>ë¶„ì„í‚¤ì›Œë“œ: {kw_badge}</span>"
            f"<span style='background:#f3f4f6; padding:6px 10px; border-radius:999px; font-size:12px;'>ë¶„ì„ê¸°ê°„: {period_badge}</span>"
            f"</div>",
            unsafe_allow_html=True,
        )
        st.markdown("---")

        # 5) AI ìš”ì•½
        st.subheader("ğŸ§  ìš”ì•½")
        sample_text, _, _ = serialize_comments_for_llm_from_file(csv_path)
        genai.configure(api_key=RotatingKeys(GEMINI_API_KEYS, "gem_key_idx").current())
        model = genai.GenerativeModel(GEMINI_MODEL, generation_config={"temperature":0.2, "max_output_tokens":GEMINI_MAX_TOKENS})
        sys = (
            "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
            "ì•„ë˜ í‚¤ì›Œë“œ/ì—”í‹°í‹°ì™€ ì§€ì •ëœ ê¸°ê°„ì˜ ëŒ“ê¸€ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í•­ëª©í™”í•˜ê³ , ê¸/ë¶€/ì¤‘ ë¹„ìœ¨ê³¼ ëŒ€í‘œ ì½”ë©˜íŠ¸(10ê°œ ë¯¸ë§Œ)ë¥¼ ì œì‹œí•˜ë¼."
        )
        payload = f"[í‚¤ì›Œë“œ]: {', '.join(keywords)}\n[ì—”í‹°í‹°]: {', '.join(entities)}\n[ê¸°ê°„(KST)]: {schema['start_iso']} ~ {schema['end_iso']}\n\n[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{sample_text}\n"
        with st.status("AI ìš”ì•½ ìƒì„± ì¤‘â€¦", expanded=False) as s:
            resp = model.generate_content([sys, payload], request_options={"timeout": GEMINI_TIMEOUT})
            st.markdown(getattr(resp, "text", ""))
            s.update(label="ìš”ì•½ ì™„ë£Œ", state="complete")

        # 6) ì •ëŸ‰ í•˜ì´ë¼ì´íŠ¸(ì‹¬í”Œ)
        st.subheader("ğŸ“Š ì •ëŸ‰ í•˜ì´ë¼ì´íŠ¸")
        try:
            tmin=tmax=None; agg={}
            for chunk in pd.read_csv(csv_path, usecols=["publishedAt"], chunksize=200_000):
                dt = pd.to_datetime(chunk["publishedAt"], errors="coerce", utc=True)
                if dt.notna().any():
                    lo, hi = dt.min(), dt.max(); tmin = lo if (tmin is None or lo < tmin) else tmin; tmax = hi if (tmax is None or hi > tmax) else tmax
                dt = dt.dt.tz_convert("Asia/Seoul").dropna()
                if dt.empty: continue
                bucket = (dt.dt.floor("H") if (tmax and tmin and (tmax-tmin).total_seconds()/3600.0 <= 48) else dt.dt.floor("D"))
                vc = bucket.value_counts()
                for t, c in vc.items(): agg[t]=agg.get(t,0)+int(c)
            if agg:
                ts = pd.Series(agg).sort_index().rename("count").reset_index().rename(columns={"index":"bucket"})
                fig = px.line(ts, x="bucket", y="count", markers=True, title="ëŒ“ê¸€ëŸ‰ ì¶”ì´ (KST)")
                st.plotly_chart(fig, use_container_width=True)
        except Exception as e:
            st.info(f"ì‹œê³„ì—´ ìƒì„± ë¶ˆê°€: {e}")

        # 7) ë‹¤ìš´ë¡œë“œ (ì „ì²´ëŒ“ê¸€ / ì „ì²´ì˜ìƒëª©ë¡)
        st.markdown("---")
        st.subheader("â¬‡ï¸ ë‹¤ìš´ë¡œë“œ")
        with open(csv_path, "rb") as f:
            st.download_button("ì „ì²´ ëŒ“ê¸€ (CSV)", data=f.read(), file_name=f"comments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")
        if not df_stats.empty:
            csv_videos = df_stats.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
            st.download_button("ì „ì²´ ì˜ìƒëª©ë¡ (CSV)", data=csv_videos, file_name=f"videos_{len(df_stats)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")

# í‘¸í„° ì•¡ì…˜
st.markdown("<div style='margin:24px 0; color:#9ca3af; font-size:12px'>Legal disclaimer</div>", unsafe_allow_html=True)

st.markdown("---")
cols = st.columns(2)
with cols[0]:
    if st.button("ğŸ”„ ì´ˆê¸°í™”", type="secondary"): st.session_state.clear(); st.rerun()
with cols[1]:
    if st.button("ğŸ§¹ ìºì‹œ/ë©”ëª¨ë¦¬ ì •ë¦¬"): st.cache_data.clear(); gc.collect(); st.success("ì •ë¦¬ ì™„ë£Œ")
