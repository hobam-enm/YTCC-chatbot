# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡ â€” ì™„ì„±ë³¸
# - ìì—°ì–´ í•œ ì¤„ â†’ (ê¸°ê°„/í‚¤ì›Œë“œ/ì˜µì…˜) í•´ì„(ì œë¯¸ë‚˜ì´) â†’ ì˜ìƒ ìˆ˜ì§‘(YouTube API) â†’ ëŒ“ê¸€ ìˆ˜ì§‘(CSV ìŠ¤íŠ¸ë¦¬ë°) â†’ AI ìš”ì•½ + ì •ëŸ‰ ì‹œê°í™”
# - ì •ëŸ‰ ë¶„ì„ ê²°ê³¼ëŠ” ì‚¬ì´ë“œë°”ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥

import streamlit as st
import pandas as pd
import os, re, gc, time, json
import requests # <-- ì¶”ê°€: Gemini API í˜¸ì¶œìš©
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from uuid import uuid4

# Google APIs
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai

# Visualization and NLP
import plotly.express as px
from plotly import graph_objects as go
import circlify
import numpy as np
from kiwipiepy import Kiwi
import stopwordsiso as stopwords

# ============== 0) í˜ì´ì§€/ê¸°ë³¸ ==============\nst.set_page_config(page_title="ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡", layout="wide")
st.title("ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡")

BASE_DIR = "/tmp"; os.makedirs(BASE_DIR, exist_ok=True)
KST = timezone(timedelta(hours=9))
# ì±—ë´‡ ì½”ë“œê°€ KST ì‹œê°„ëŒ€ ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìš”í•œ í•¨ìˆ˜ë“¤
def now_kst(): return datetime.now(tz=KST)
def to_iso_kst(dt: datetime) -> str:
    if dt.tzinfo is None: dt = dt.replace(tzinfo=KST)
    return dt.astimezone(KST).isoformat(timespec="seconds")
def kst_to_rfc3339_utc(dt_kst: datetime) -> str:
    if dt_kst.tzinfo is None: dt_kst = dt_kst.replace(tzinfo=KST)
    return dt_kst.astimezone(timezone.utc).isoformat().replace('+00:00', 'Z')
def parse_youtube_url(url: str) -> str:
    if "youtu.be" in url: return urlparse(url).path[1:]
    if "youtube.com" in url:
        query = parse_qs(urlparse(url).query)
        if 'v' in query: return query['v'][0]
    return ""
def safe_rerun():
    try: st.rerun()
    except: pass


# ============== 1) API Key ê´€ë¦¬ ==============

@st.cache_resource
def _get_available_keys(key_name: str, default_fallback: list) -> list:
    """Streamlit Secretsì—ì„œ API í‚¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    try:
        # Secretsì—ì„œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í‚¤ë¥¼ ë¡œë“œ
        keys = st.secrets.get(key_name, default_fallback)
        if not isinstance(keys, list):
            # ë‹¨ì¼ í‚¤ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì‹œë„
            if isinstance(keys, str) and keys:
                return [keys]
            raise ValueError(f"Secretsì˜ '{key_name}' ê°’ì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        # ì‚¬ìš©ìê°€ ì œê³µí•œ í‚¤ ëª©ë¡ì„ secretsì—ì„œ ë¡œë“œí•˜ë„ë¡ ì„¤ì • (AIzaSyë¡œ ì‹œì‘í•˜ëŠ” ìœ íš¨ í‚¤ë§Œ)
        return [k for k in keys if k and k.startswith('AIzaSy')]
    except Exception as e:
        # st.secrets ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ (ë¡œì»¬ í™˜ê²½ ë“±)
        print(f"[{key_name}] Secrets ë¡œë”© ì‹¤íŒ¨: {e}")
        return default_fallback

# --- API Keys ë¡œë”© (Secrets/í™˜ê²½ ë³€ìˆ˜ ìš°ì„ ) ---
# ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€í•˜ì—¬ Secretsì—ì„œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
YT_API_KEYS = _get_available_keys("YT_API_KEYS", [])
GEMINI_API_KEYS = _get_available_keys("GEMINI_API_KEYS", [])

if not YT_API_KEYS:
    st.error("ğŸš¨ YouTube API Keyë¥¼ secrets.tomlì— 'YT_API_KEYS' ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
if not GEMINI_API_KEYS:
    st.error("ğŸš¨ Gemini API Keyë¥¼ secrets.tomlì— 'GEMINI_API_KEYS' ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# Kiwi ê°ì²´ëŠ” ë¬´ê±°ìš°ë¯€ë¡œ ìºì‹œ ë¦¬ì†ŒìŠ¤ë¡œ ê´€ë¦¬
@st.cache_resource
def get_kiwi():
    return Kiwi(model_type='sbg', space_in_eojeol=True)

# ê¸°íƒ€ ìƒíƒœ ì´ˆê¸°í™”
if "last_video_id" not in st.session_state: st.session_state.last_video_id = ""
if "last_comments_file" not in st.session_state: st.session_state.last_comments_file = None
if "df_for_sidebar" not in st.session_state: st.session_state.df_for_sidebar = None # ì‚¬ì´ë“œë°” ì‹œê°í™”ìš© í•„í„°ë§ëœ DF
if "last_video_info" not in st.session_state: st.session_state.last_video_info = None # ì‚¬ì´ë“œë°” ì˜ìƒ ì •ë³´ìš©
if "last_schema" not in st.session_state: st.session_state.last_schema = None
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ğŸ¤– **ë¶„ì„í•  YouTube ì˜ìƒ URL**ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. (ì˜ˆ: 'ì‹ ì œí’ˆ ë¦¬ë·° ì˜ìƒ ëŒ“ê¸€ 1ì£¼ì¼ì¹˜ ìš”ì•½í•´ì¤˜')"}]

# ============== 2) YouTube API ë¡œì§ (ytccai_cloud ë¡œì§ ê¸°ë°˜) ==============

def _get_youtube_service(key: str):
    """YouTube API ì„œë¹„ìŠ¤ ë¹Œë“œ."""
    try: return build('youtube', 'v3', developerKey=key, cache_discovery=False)
    except Exception: return None

def _rotate_key(api_keys: list) -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ì¤‘ í•˜ë‚˜ë¥¼ ìˆœí™˜í•˜ì—¬ ë°˜í™˜."""
    if not api_keys: return None
    key = api_keys.pop(0)
    api_keys.append(key)
    return key

def get_video_info(video_id: str):
    """ë¹„ë””ì˜¤ ì œëª©, ì±„ë„, ê²Œì‹œì¼ ë“±ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    api_keys = YT_API_KEYS[:]
    for _ in range(len(api_keys)):
        key = _rotate_key(api_keys)
        if not key: continue
        try:
            youtube = _get_youtube_service(key)
            if not youtube: continue
            
            response = youtube.videos().list(
                part="snippet,contentDetails,statistics",
                id=video_id
            ).execute()
            
            if response.get("items"):
                item = response["items"][0]
                published_at_utc = datetime.fromisoformat(item["snippet"]["publishedAt"].replace('Z', '+00:00'))
                return {
                    "title": item["snippet"]["title"],
                    "channelTitle": item["snippet"]["channelTitle"],
                    "published_at_kst": to_iso_kst(published_at_utc.astimezone(KST)),
                    "viewCount": int(item["statistics"].get("viewCount", 0)),
                    "likeCount": int(item["statistics"].get("likeCount", 0)),
                    "commentCount": int(item["statistics"].get("commentCount", 0))
                }
        except HttpError as e:
            if 'quotaExceeded' in str(e) or 'keyInvalid' in str(e):
                st.warning(f"YouTube API í‚¤ í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ë¬´íš¨: {key[:10]}...")
                continue
            st.error(f"ë¹„ë””ì˜¤ ì •ë³´ ë¡œë“œ ì˜¤ë¥˜: {e}")
            break
        except Exception as e:
            st.error(f"ë¹„ë””ì˜¤ ì •ë³´ ë¡œë“œ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
            break
    return None

def fetch_comments_to_csv(video_id: str, max_results: int = 10000) -> str or None:
    """
    YouTube APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  CSV íŒŒì¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì €ì¥í•©ë‹ˆë‹¤.
    (ytccai_cloud.pyì˜ ë©”ëª¨ë¦¬ ìµœì í™” ë¡œì§ ë°˜ì˜)
    """
    
    # 1. íŒŒì¼ ê²½ë¡œ ì„¤ì •
    filename = os.path.join(BASE_DIR, f"comments_{video_id}_{now_kst().strftime('%Y%m%d%H%M%S')}.csv")
    
    # 2. API í˜¸ì¶œ ë¡œì§
    api_keys = YT_API_KEYS[:]
    total_comments = 0
    next_page_token = None
    
    try:
        with open(filename, 'w', encoding='utf-8-sig') as f:
            # CSV í—¤ë” ì‘ì„± (ytccai_cloud.pyì˜ ì»¬ëŸ¼ ì‚¬ìš©)
            header = "comment_id,author,published_at,text,like_count,reply_count,video_id,parent_id\n"
            f.write(header)
            
            # ìˆ˜ì§‘ ë£¨í”„
            for _ in range(20): # ìµœëŒ€ 20í˜ì´ì§€ (ì•½ 1ë§Œ ê°œ)
                key = _rotate_key(api_keys)
                if not key:
                    st.error("ëª¨ë“  YouTube API í‚¤ê°€ í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                    
                youtube = _get_youtube_service(key)
                if not youtube: continue
                
                request = youtube.commentThreads().list(
                    part="snippet,replies",
                    videoId=video_id,
                    maxResults=50,
                    pageToken=next_page_token,
                    order="time"
                )
                response = request.execute()
                
                # ëŒ“ê¸€ ì“°ê¸°
                for item in response.get("items", []):
                    top_level_comment = item["snippet"]["topLevelComment"]["snippet"]
                    comment_data = {
                        "comment_id": item["snippet"]["topLevelComment"]["id"],
                        "author": top_level_comment["authorDisplayName"],
                        "published_at": top_level_comment["publishedAt"],
                        "text": top_level_comment["textDisplay"].replace('\n', ' ').replace('\r', ' '),
                        "like_count": top_level_comment["likeCount"],
                        "reply_count": item["snippet"].get("totalReplyCount", 0),
                        "video_id": video_id,
                        "parent_id": None
                    }
                    
                    # CSV ë¼ì¸ ìƒì„±
                    # í•„ë“œ ê°’ì— ì½¤ë§ˆê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, CSV ê·œê²©ì— ë§ê²Œ í…ìŠ¤íŠ¸ëŠ” í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ëŠ” ê²ƒì´ ì•ˆì „í•˜ë‚˜,
                    # ê¸°ì¡´ ë¡œì§ì„ ìœ ì§€í•˜ê³  í…ìŠ¤íŠ¸ ë‚´ í°ë”°ì˜´í‘œë§Œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                    line_values = [str(comment_data[col]).replace('"', '""') for col in header.strip().split(',')]
                    # text í•„ë“œë§Œì€ í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„œ CSV ì•ˆì „ì„± í™•ë³´ (í•„ìˆ˜)
                    line_values[3] = f'"{line_values[3]}"' 
                    
                    f.write(','.join(line_values) + '\n')
                    total_comments += 1

                next_page_token = response.get('nextPageToken')
                if not next_page_token or total_comments >= max_results:
                    break
            
        return filename if total_comments > 0 else None
        
    except HttpError as e:
        if 'commentsDisabled' in str(e):
            st.error("âš ï¸ ì´ ì˜ìƒì€ ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ API ì˜¤ë¥˜ ë°œìƒ: {e}")
        if os.path.exists(filename): os.remove(filename)
        return None
    except Exception as e:
        st.error(f"ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if os.path.exists(filename): os.remove(filename)
        return None

# ============== 3) Gemini ë¡œì§ ==============

def _rotate_gemini_key():
    """Gemini API í‚¤ ìˆœí™˜."""
    return _rotate_key(GEMINI_API_KEYS)

def parse_user_query_to_schema(user_query: str, last_video_id: str) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìœ íŠœë¸Œ ë¶„ì„ì— í•„ìš”í•œ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ytcc_chatbot.pyì˜ í•µì‹¬ íŒŒì‹± ë¡œì§ ë°˜ì˜)
    """
    key = _rotate_gemini_key()
    if not key:
        return {"error": "Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ë¶„ì„ ëª…ë ¹ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ytcc_chatbot.py ë¡œì§ ê¸°ë°˜)
    schema = {
        "type": "OBJECT",
        "properties": {
            "video_id": {
                "type": "STRING",
                "description": f"ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ìœ íŠœë¸Œ ì˜ìƒ ID. ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ '{last_video_id}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. URLì´ ì•„ë‹Œ ID ìì²´ì—¬ì•¼ í•©ë‹ˆë‹¤."
            },
            "analysis_type": {
                "type": "STRING",
                "description": "'FULL_ANALYSIS' (ìƒˆë¡œìš´ ì˜ìƒ/ê¸°ê°„ ìš”ì²­), 'CHAT_FOLLOW_UP' (ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸), 'SIMPLE_QA' (ì¼ë°˜ ì§ˆë¬¸) ì¤‘ í•˜ë‚˜."
            },
            "start_iso": {
                "type": "STRING",
                "description": "ë¶„ì„ ì‹œì‘ ë‚ ì§œ (ISO 8601 í˜•ì‹). ì˜ˆ: '2024-01-01T00:00:00+09:00'"
            },
            "end_iso": {
                "type": "STRING",
                "description": "ë¶„ì„ ì¢…ë£Œ ë‚ ì§œ (ISO 8601 í˜•ì‹). ì˜ˆ: '2024-03-31T23:59:59+09:00'"
            },
            "keywords": {
                "type": "ARRAY",
                "items": {"type": "STRING"},
                "description": "ì‚¬ìš©ìê°€ íŠ¹ë³„íˆ ì°¾ìœ¼ë ¤ëŠ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: 'ê°€ê²©', 'ì„±ëŠ¥', 'ë²„ê·¸')."
            },
            "entities": {
                "type": "ARRAY",
                "items": {"type": "STRING"},
                "description": "í‚¤ì›Œë“œ ì™¸ì— ê°ì§€ëœ ë³´ì¡° ì£¼ì œë‚˜ ì—”í‹°í‹° (ì˜ˆ: 'ê°¤ëŸ­ì‹œ S24', 'ì¸ê³µì§€ëŠ¥')."
            }
        },
        "required": ["analysis_type"]
    }
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    today = now_kst()
    a_month_ago = today - timedelta(days=30)
    
    current_schema = {
        "video_id": last_video_id,
        "start_iso": to_iso_kst(a_month_ago.replace(hour=0, minute=0, second=0, microsecond=0)),
        "end_iso": to_iso_kst(today.replace(hour=23, minute=59, second=59, microsecond=0)),
        "keywords": [],
        "entities": []
    }
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ ì¶”ê°€ (ê°„ê²°í•˜ê²Œ)
    # history_context = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages[-4:]]) # ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    
    system_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ì ìš”ì²­ì„ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì‹œìŠ¤í…œì´ ì´í•´í•  ìˆ˜ ìˆëŠ” JSON ëª…ë ¹ì–´ë¡œ ë³€í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, í•„ìš”í•œ ê²½ìš° ë¹„ë””ì˜¤ ID, ê¸°ê°„, í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶¥ë‹ˆë‹¤. "
        f"ë§ˆì§€ë§‰ìœ¼ë¡œ ë¶„ì„í•œ ì˜ìƒ IDëŠ” '{last_video_id}'ì…ë‹ˆë‹¤. í˜„ì¬ ë‚ ì§œëŠ” {today.strftime('%Yë…„ %mì›” %dì¼')}ì…ë‹ˆë‹¤. "
        f"ìƒˆë¡œìš´ URLì´ ê°ì§€ë˜ë©´ 'video_id'ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , ë¶„ì„ ìš”ì²­ì´ë©´ 'FULL_ANALYSIS'ë¡œ ì„¤ì •í•˜ì„¸ìš”. "
        "ê¸°ì¡´ ë°ì´í„°ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ì´ë©´ 'CHAT_FOLLOW_UP'ë¡œ ì„¤ì •í•˜ê³ , ì¼ë°˜ì ì¸ ì§ˆë¬¸ì€ 'SIMPLE_QA'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. "
        "ê¸°ê°„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì§€ë‚œ 30ì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤."
    )
    
    try:
        payload = {
            "contents": [{ "parts": [{ "text": user_query }] }],
            "systemInstruction": { "parts": [{ "text": system_prompt }] },
            "generationConfig": {
                "responseMimeType": "application/json",
                "responseSchema": schema
            }
        }
        
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={key}"
        
        # Exponential Backoff
        for attempt in range(3):
            response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            if response.status_code == 200:
                json_string = response.json()["candidates"][0]["content"]["parts"][0]["text"]
                parsed = json.loads(json_string)
                # URLì´ ìˆë‹¤ë©´ IDë¡œ ë³€í™˜í•˜ì—¬ í• ë‹¹
                if parsed.get("video_id"):
                    if 'youtube.com' in parsed['video_id'] or 'youtu.be' in parsed['video_id']:
                        parsed['video_id'] = parse_youtube_url(parsed['video_id'])
                return {**current_schema, **parsed}
            elif response.status_code == 429 and attempt < 2:
                time.sleep(2 ** attempt)
                continue
            else:
                st.error(f"íŒŒì‹± ëª¨ë¸ API í˜¸ì¶œ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
                # st.json(response.json()) # ë””ë²„ê¹… ì •ë³´ ì œê±°
                return {"error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}"}
    except Exception as e:
        st.error(f"íŒŒì‹± ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"error": str(e)}

def synthesize_response(query: str, analysis_data: str, video_info: dict, schema: dict) -> str:
    """ë¶„ì„ ë°ì´í„°ì™€ ì›ë˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    key = _rotate_gemini_key()
    if not key: return "Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."

    system_prompt = (
        "ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì „ë¬¸ê°€ì´ì ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ì œê³µëœ 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°'ì™€ 'ë¹„ë””ì˜¤ ì •ë³´'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í†µì°°ë ¥ ìˆê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "
        "ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ì§€ ì•Šê³ , í•µì‹¬ ìš”ì•½ê³¼ í†µê³„ë¥¼ í¬í•¨í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”. "
        "ë§Œì•½ 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°'ê°€ ë¹„ì–´ ìˆë‹¤ë©´, ë¹„ë””ì˜¤ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    )
    
    video_info_str = json.dumps(video_info, ensure_ascii=False, indent=2) if video_info else "N/A"
    
    user_prompt = (
        f"ì‚¬ìš©ìì˜ ì§ˆë¬¸: '{query}'\n"
        f"ë¶„ì„ ìš”ì²­ ìŠ¤í‚¤ë§ˆ: {json.dumps(schema, ensure_ascii=False)}\n"
        f"ë¹„ë””ì˜¤ ê¸°ë³¸ ì •ë³´:\n{video_info_str}\n"
        f"ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼ ë°ì´í„°:\n{analysis_data}\n\n"
        "ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. íŠ¹íˆ í‚¤ì›Œë“œë‚˜ ê¸°ê°„ì´ ì§€ì •ë˜ì—ˆë‹¤ë©´ ê·¸ ê²°ê³¼ë¥¼ ê°•ì¡°í•˜ì„¸ìš”."
    )

    try:
        payload = {
            "contents": [{ "parts": [{ "text": user_prompt }] }],
            "systemInstruction": { "parts": [{ "text": system_prompt }] }
        }
        
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={key}"
        
        for attempt in range(3):
            response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            if response.status_code == 200:
                return response.json()["candidates"][0]["content"]["parts"][0]["text"]
            elif response.status_code == 429 and attempt < 2:
                time.sleep(2 ** attempt)
                continue
            else:
                return f"âš ï¸ ë‹µë³€ í•©ì„± ëª¨ë¸ API í˜¸ì¶œ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}"
    except Exception as e:
        return f"âš ï¸ ë‹µë³€ í•©ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ============== 4) ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ë¡œì§ ==============

def get_filtered_comments_df(comments_file: str, schema: dict) -> pd.DataFrame:
    """ëŒ“ê¸€ CSV íŒŒì¼ì„ ì½ê³ , ê¸°ê°„ ë° í‚¤ì›Œë“œì— ë”°ë¼ í•„í„°ë§í•©ë‹ˆë‹¤."""
    if not comments_file or not os.path.exists(comments_file):
        return None
    
    # 1. CSV ë¡œë“œ (ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™” ìœ„í•´ ChunkSizeë¥¼ 10ë§Œìœ¼ë¡œ)
    df_list = []
    try:
        # 'text' ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ëª…í™•íˆ ì§€ì •í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€
        for chunk in pd.read_csv(comments_file, chunksize=100000, encoding='utf-8-sig', dtype={'comment_id': str, 'parent_id': str, 'text': str}):
            df_list.append(chunk)
        df = pd.concat(df_list, ignore_index=True)
    except Exception as e:
        st.error(f"CSV íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None
    
    if df.empty: return df

    # 2. ê¸°ê°„ í•„í„°ë§
    start_dt = datetime.fromisoformat(schema['start_iso'])
    end_dt = datetime.fromisoformat(schema['end_iso'])
    
    df['published_at'] = pd.to_datetime(df['published_at'], utc=True)
    # KSTë¡œ ë³€í™˜ í›„ í•„í„°ë§
    df['published_at_kst'] = df['published_at'].dt.tz_convert(KST)
    
    df_filtered = df[(df['published_at_kst'] >= start_dt) & (df['published_at_kst'] <= end_dt)]
    
    # 3. í‚¤ì›Œë“œ í•„í„°ë§
    keywords = [k for k in schema.get('keywords', []) if k]
    if keywords:
        # `text`ê°€ NaNì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ `fillna('')`ë¥¼ ì¶”ê°€
        pattern = '|'.join(re.escape(k) for k in keywords)
        df_filtered = df_filtered[df_filtered['text'].fillna('').str.contains(pattern, case=False, na=False)]

    return df_filtered.reset_index(drop=True)

def generate_analysis_summary(df: pd.DataFrame, schema: dict) -> str:
    """í•„í„°ë§ëœ DataFrameì„ ê¸°ë°˜ìœ¼ë¡œ AIì—ê²Œ ì „ë‹¬í•  ë¶„ì„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if df.empty:
        return "í•„í„°ë§ëœ ëŒ“ê¸€ ë°ì´í„°ê°€ ì—†ì–´ ì •ëŸ‰ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê¸°ê°„ ë˜ëŠ” í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜)"
    
    summary = f"**[ë¶„ì„ ê²°ê³¼ ìš”ì•½]**\n"
    summary += f"- ë¶„ì„ ëŒ€ìƒ ëŒ“ê¸€ ìˆ˜: {len(df)}ê°œ\n"
    summary += f"- ê³ ìœ  ì‘ì„±ì ìˆ˜: {df['author'].nunique()}ëª…\n"
    
    # ìµœë‹¤ ì¢‹ì•„ìš” ëŒ“ê¸€
    top_liked = df.sort_values(by='like_count', ascending=False).head(1)
    if not top_liked.empty:
        summary += f"- ìµœë‹¤ ì¢‹ì•„ìš” ëŒ“ê¸€ ({top_liked['like_count'].iloc[0]}ê°œ, ì‘ì„±ì: {top_liked['author'].iloc[0]}): \"{top_liked['text'].iloc[0][:100].replace('\n', ' ')}...\"\n"

    # í‚¤ì›Œë“œ/ì—”í‹°í‹° ë¹ˆë„ ë¶„ì„ (ëª…ì‚¬ ì¶”ì¶œ)
    kiwi = get_kiwi()
    stop_words = stopwords.stopwords(["ko"])
    
    all_text = ' '.join(df['text'].dropna())
    tokens = kiwi.tokenize(all_text)
    
    nouns = [t.form for t in tokens if t.tag.startswith('N') and len(t.form) > 1 and t.form not in stop_words]
    
    # ì±—ë´‡ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë„ ì¹´ìš´í„°ì— í¬í•¨í•˜ì—¬ ì¤‘ìš”ë„ë¥¼ ë†’ì„
    user_keywords = [k for k in schema.get('keywords', []) if k]
    nouns.extend(user_keywords * 10) # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    
    noun_counts = Counter(nouns).most_common(20)
    
    summary += "\n**[ì£¼ìš” ëª…ì‚¬ ë¹ˆë„ (Top 20)]**\n"
    summary += ', '.join([f"{n[0]}({n[1]})" for n in noun_counts]) + "\n"

    # ëŒ“ê¸€ ìƒ˜í”Œ (AI ë‹µë³€ ë§¥ë½ ìœ ì§€ë¥¼ ìœ„í•´ ìµœê·¼ 10ê°œ)
    sample_comments = df['text'].tail(10).str.cat(sep='\n---END---\n').replace('\n', ' ')
    summary += "\n**[ëŒ“ê¸€ ìƒ˜í”Œ (ìµœê·¼ 10ê°œ)]**\n"
    summary += sample_comments
    
    return summary

# ì‹œê°í™” í•¨ìˆ˜ (UI êµ¬ì„± ì‹œ ì‚¬ìš©)
def generate_keyword_chart(df: pd.DataFrame, title: str):
    """í‚¤ì›Œë“œ ë²„ë¸” ì°¨íŠ¸ ìƒì„± (Top 30 ëª…ì‚¬ ê¸°ë°˜)."""
    if df.empty: return
    kiwi = get_kiwi()
    stop_words = stopwords.stopwords(["ko"])
    
    all_text = ' '.join(df['text'].dropna())
    tokens = kiwi.tokenize(all_text)
    nouns = [t.form for t in tokens if t.tag.startswith('N') and len(t.form) > 1 and t.form not in stop_words]
    noun_counts = Counter(nouns).most_common(30)
    
    if not noun_counts: return
    
    data = pd.DataFrame(noun_counts, columns=['keyword', 'count'])
    
    # ë²„ë¸” ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ (circlify) ì‚¬ìš©
    circles = circlify.circlify(
        data['count'].tolist(),
        target_enclosure=circlify.Circle(x=0, y=0, r=1)
    )
    
    fig = go.Figure()
    
    for circle, (keyword, count) in zip(circles, noun_counts):
        if circle.level == 1:
            fig.add_shape(type="circle",
                xref="x", yref="y",
                x0=circle.x - circle.r, y0=circle.y - circle.r,
                x1=circle.x + circle.r, y1=circle.y + circle.r,
                fillcolor=px.colors.qualitative.Plotly[noun_counts.index((keyword, count)) % len(px.colors.qualitative.Plotly)],
                line_color="rgba(0,0,0,0)",
            )
            
            fig.add_annotation(
                x=circle.x, y=circle.y,
                text=f"{keyword}<br>({count})",
                showarrow=False,
                font=dict(size=min(max(10, count * 0.05), 18), color="white"),
                align="center",
            )
            
    fig.update_layout(
        title=title,
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        plot_bgcolor='rgba(0,0,0,0)',
        paper_bgcolor='rgba(0,0,0,0)',
        height=400,
        margin=dict(l=20, r=20, t=50, b=20)
    )
    
    st.plotly_chart(fig, use_container_width=True)

def render_quantitative_analysis(df_filtered: pd.DataFrame, video_info: dict, schema: dict):
    """
    ì‚¬ì´ë“œë°”ì— ì •ëŸ‰ ë¶„ì„ ê²°ê³¼, í•„í„°ë§ ì •ë³´ ë° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ë Œë”ë§í•©ë‹ˆë‹¤.
    """
    if df_filtered is None or df_filtered.empty or schema is None:
        st.subheader("ğŸ“Š ì •ëŸ‰ ë¶„ì„ ëŒ€ê¸° ì¤‘")
        st.info("ëŒ“ê¸€ì„ ë¶„ì„í•˜ë©´ ì—¬ê¸°ì— ê²°ê³¼(ì°¨íŠ¸, í†µê³„)ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
        return

    st.subheader("ğŸ“Š ì •ëŸ‰ ë¶„ì„ ê²°ê³¼")
    
    # 1. í•„í„°ë§ ì •ë³´
    with st.expander("ë¶„ì„ ëŒ€ìƒ ìš”ì•½", expanded=True):
        if video_info:
            st.markdown(f"**ì˜ìƒ ì œëª©:** `{video_info['title'][:50]}...`")
            st.markdown(f"**ì±„ë„:** `{video_info['channelTitle']}`")
            st.markdown(f"**ì› ëŒ“ê¸€ ìˆ˜:** `{video_info.get('commentCount', 'N/A'):,}`ê°œ")
        
        st.markdown("---")
        st.markdown(f"**í•„í„°ë§ ê¸°ê°„:** `{schema['start_iso'].split('T')[0]} ~ {schema['end_iso'].split('T')[0]}`")
        keywords = [k for k in schema.get('keywords', []) if k]
        st.markdown(f"**ì ìš© í‚¤ì›Œë“œ:** {', '.join(keywords) or '(ì „ì²´)'}")
        st.markdown(f"**ë¶„ì„ ëŒ€ìƒ ëŒ“ê¸€ ìˆ˜:** **{len(df_filtered):,}**ê°œ")
    
    # 2. í‚¤ì›Œë“œ ì°¨íŠ¸
    generate_keyword_chart(df_filtered, "ì£¼ìš” í‚¤ì›Œë“œ ë²„ë¸”")
    
    # 3. ë‹¤ìš´ë¡œë“œ (í•„í„°ë§ëœ ëŒ“ê¸€)
    st.markdown("---")
    csv_filtered = df_filtered.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        "â¬‡ï¸ í•„í„°ë§ëœ ëŒ“ê¸€ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_filtered,
        file_name=f"filtered_comments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )

# ============== 5) ë©”ì¸ ì±—ë´‡ ë£¨í”„ ==============

# --- Chat Display ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Chat Input Handler ---
if prompt := st.chat_input("YouTube URL ë˜ëŠ” ë¶„ì„ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        
        # 2. Gemini íŒŒì‹± (ì§ˆë¬¸ -> Schema ëª…ë ¹ì–´)
        with st.spinner("AIê°€ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ë¶„ì„ ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            parsed_schema = parse_user_query_to_schema(prompt, st.session_state.last_video_id)
            st.session_state.last_schema = parsed_schema # ìŠ¤í‚¤ë§ˆ ì €ì¥ (ì‚¬ì´ë“œë°” ì¶œë ¥ìš©)

        if parsed_schema.get("error"):
            st.error(f"íŒŒì‹± ì˜¤ë¥˜: {parsed_schema['error']}")
            st.session_state.messages.append({"role": "assistant", "content": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {parsed_schema['error']}"})
            safe_rerun()

        video_id = parsed_schema.get("video_id", "")
        analysis_type = parsed_schema.get("analysis_type", "SIMPLE_QA")
        
        # 3. ë°ì´í„° ì²˜ë¦¬ ë° AI ë‹µë³€ ìƒì„±
        
        df_filtered = None
        video_info = st.session_state.last_video_info # ê¸°ì¡´ ì •ë³´ ì‚¬ìš© ì‹œë„
        
        if analysis_type == "SIMPLE_QA":
            # ì¼ë°˜ QAë§Œ ìš”ì²­ë°›ì€ ê²½ìš°
            response = synthesize_response(prompt, "N/A", video_info, parsed_schema)
            st.markdown(response)
        
        elif analysis_type == "FULL_ANALYSIS":
            
            # --- 3-1. ë¹„ë””ì˜¤ ì •ë³´ ë° ë°ì´í„° ìˆ˜ì§‘ ---
            if not video_id:
                st.warning("ë¶„ì„í•  ì˜ìƒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")
                st.session_state.messages.append({"role": "assistant", "content": "ë¶„ì„í•  ì˜ìƒ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."})
                safe_rerun()
            
            with st.spinner(f"ë¹„ë””ì˜¤ ì •ë³´ ë¡œë“œ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘ (ID: {video_id})..."):
                
                # ë¹„ë””ì˜¤ ì •ë³´ ë¡œë“œ ë° ìƒíƒœ ì €ì¥
                video_info = get_video_info(video_id)
                if not video_info:
                    st.error("ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ID ë˜ëŠ” API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    st.session_state.messages.append({"role": "assistant", "content": "ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ID ë˜ëŠ” API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."})
                    safe_rerun()
                st.session_state.last_video_info = video_info # <-- ìƒíƒœ ì €ì¥
                
                # ëŒ“ê¸€ ìˆ˜ì§‘ (ìƒˆë¡œìš´ ì˜ìƒì¸ ê²½ìš°)
                if video_id != st.session_state.last_video_id:
                    comments_file = fetch_comments_to_csv(video_id)
                    if not comments_file:
                        st.error("ëŒ“ê¸€ì„ ìˆ˜ì§‘í•  ìˆ˜ ì—†ê±°ë‚˜ ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        st.session_state.messages.append({"role": "assistant", "content": "ëŒ“ê¸€ì„ ìˆ˜ì§‘í•  ìˆ˜ ì—†ê±°ë‚˜ ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."})
                        safe_rerun()
                    
                    st.session_state.last_video_id = video_id
                    st.session_state.last_comments_file = comments_file
                else:
                    comments_file = st.session_state.last_comments_file
            
            # --- 3-2. ë°ì´í„° í•„í„°ë§ ë° ë¶„ì„ ---
            with st.spinner("ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  ë¶„ì„í•˜ëŠ” ì¤‘..."):
                df_filtered = get_filtered_comments_df(comments_file, parsed_schema)
                st.session_state.df_for_sidebar = df_filtered # <-- ì‚¬ì´ë“œë°”ìš© DF ì €ì¥
                analysis_data = generate_analysis_summary(df_filtered, parsed_schema)
            
            # --- 3-3. AI ë‹µë³€ í•©ì„± (ì‹œê°í™”ëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì²˜ë¦¬) ---
            with st.spinner("AIê°€ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                final_response = synthesize_response(prompt, analysis_data, video_info, parsed_schema)
                st.markdown(final_response)

        
        elif analysis_type == "CHAT_FOLLOW_UP":
            
            # --- 3-1. ê¸°ì¡´ ë°ì´í„° ì¬í™œìš© ë° í•„í„°ë§ ---
            if not st.session_state.last_comments_file:
                st.warning("ì´ì „ì— ë¶„ì„ëœ ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. URLê³¼ í•¨ê»˜ ë¶„ì„ ìš”ì²­ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”.")
                st.session_state.messages.append({"role": "assistant", "content": "ì´ì „ì— ë¶„ì„ëœ ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. URLê³¼ í•¨ê»˜ ë¶„ì„ ìš”ì²­ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”."})
                safe_rerun()
                
            comments_file = st.session_state.last_comments_file
            video_info = st.session_state.last_video_info # ì´ì „ì— ì €ì¥ëœ ì •ë³´ ì‚¬ìš©

            with st.spinner("ê¸°ì¡´ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  ì¶”ê°€ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                df_filtered = get_filtered_comments_df(comments_file, parsed_schema)
                st.session_state.df_for_sidebar = df_filtered # <-- ì‚¬ì´ë“œë°”ìš© DF ì €ì¥
                analysis_data = generate_analysis_summary(df_filtered, parsed_schema)

            # --- 3-2. AI ë‹µë³€ í•©ì„± ---
            with st.spinner("AIê°€ ì¶”ê°€ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                final_response = synthesize_response(prompt, analysis_data, video_info, parsed_schema)
                st.markdown(final_response)
        
# ============== 6) ì‚¬ì´ë“œë°” ë Œë”ë§ ==============

with st.sidebar:
    st.header("âš™ï¸ ë¶„ì„ ìƒíƒœ ë° ì •ëŸ‰ ê²°ê³¼")
    
    # 1. ì •ëŸ‰ ë¶„ì„ ê²°ê³¼
    # ì‚¬ì´ë“œë°”ì—ì„œ DFë¥¼ ì‚¬ìš©í•´ ì°¨íŠ¸ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.
    render_quantitative_analysis(
        st.session_state.df_for_sidebar,
        st.session_state.last_video_info,
        st.session_state.last_schema
    )
    
    st.markdown("---")
    
    # 2. í‚¤/ì„¸ì…˜ ì •ë³´
    st.subheader("í‚¤/ì„¸ì…˜ ì •ë³´")
    st.write(f"YT Keys: {len(YT_API_KEYS)}ê°œ / Gemini Keys: {len(GEMINI_API_KEYS)}ê°œ")
    st.markdown(f"**Last Video ID:** `{st.session_state.last_video_id or '(ì—†ìŒ)'}`")

    # 3. ë¶„ì„ ìŠ¤í‚¤ë§ˆ ì •ë³´
    st.subheader("ë§ˆì§€ë§‰ ë¶„ì„ ìŠ¤í‚¤ë§ˆ")
    if st.session_state.last_schema:
        s = st.session_state.last_schema
        st.markdown(f"- **ìœ í˜•:** `{s.get('analysis_type', 'N/A')}`")
        st.markdown(f"- **ê¸°ê°„:** `{s.get('start_iso', 'N/A').split('T')[0]} ~ {s.get('end_iso', 'N/A').split('T')[0]}`")
        keywords = s.get('keywords', [])
        st.markdown(f"- **í‚¤ì›Œë“œ:** {', '.join(keywords) or '(ì—†ìŒ)'}")
    else:
        st.caption("íŒŒì‹± ëŒ€ê¸° ì¤‘...")
