# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡: ìì—°ì–´ ì§ˆë¬¸ ì²˜ë¦¬, ë°ì´í„° ìˆ˜ì§‘ ë° AI ë‹µë³€ ìƒì„±

import streamlit as st
import pandas as pd
import os
import re
import time
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta, timezone
import requests
import json

# === 0. í˜ì´ì§€ ì„¤ì • (ë°˜ë“œì‹œ ëª¨ë“  st.xxx í˜¸ì¶œ ì¤‘ ê°€ì¥ ë¨¼ì € ì™€ì•¼ í•¨) ===
st.set_page_config(page_title="ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡", layout="wide")

# ===== 1. ê¸°ë³¸ ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ytccai_cloud.py ë° ytcc_chatbot.py ê¸°ë°˜) =====

# ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ Streamlit Secretsì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.
try:
    # ğŸ”‘ Streamlit secretsì—ì„œ API í‚¤ ë¡œë“œ
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except (KeyError, AttributeError, FileNotFoundError):
    # secretsì— ì—†ëŠ” ê²½ìš° í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ ì‚¬ìš© (fallback)
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")
    if not GEMINI_API_KEY:
        st.warning("âš ï¸ Streamlit Secrets(`[secrets] GEMINI_API_KEY`) ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì±—ë´‡ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# YouTube API í‚¤ë„ í•„ìš”í•˜ì§€ë§Œ, ì´ ì˜ˆì œì—ì„œëŠ” ëŒ“ê¸€ ìˆ˜ì§‘ ë¡œì§ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.
# ì‹¤ì œ ytccai_cloud.py ì—ì„œëŠ” build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

BASE_DIR = "/tmp"; os.makedirs(BASE_DIR, exist_ok=True)
KST = timezone(timedelta(hours=9))

def now_kst(): 
    return datetime.now(tz=KST)

# Streamlit ì¬ì‹¤í–‰ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def safe_rerun():
    """Streamlit Cloud í™˜ê²½ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ í”¼í•˜ë©° ì¬ì‹¤í–‰ì„ ì‹œë„í•©ë‹ˆë‹¤."""
    try:
        st.rerun()
    except:
        pass

# YouTube URL íŒŒì‹± í•¨ìˆ˜
def parse_youtube_url(url: str) -> str:
    """YouTube URLì—ì„œ video IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if "youtu.be" in url:
        return urlparse(url).path[1:]
    if "youtube.com" in url:
        query = parse_qs(urlparse(url).query)
        if 'v' in query:
            return query['v'][0]
    return ""

# ëŒ“ê¸€ ë°ì´í„°ë¥¼ ê°€ì§œë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í•¨ìˆ˜
# ì‹¤ì œë¡œëŠ” ytccai_cloud.pyì˜ fetch_comments_to_csv ë¡œì§ì„ í†µí•´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì•¼ í•©ë‹ˆë‹¤.
def mock_fetch_comments(video_id: str, count: int = 100) -> pd.DataFrame:
    """ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ ê°€ìƒì˜ ëŒ“ê¸€ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜)"""
    st.info(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (Video ID: {video_id}) - ì‹¤ì œë¡œëŠ” YouTube APIê°€ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    time.sleep(2) # API í˜¸ì¶œ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
    
    data = []
    # ë¶„ì„ì— í•„ìš”í•œ ìµœì†Œí•œì˜ ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    keywords = ["ì‹ ì œí’ˆ", "ë¹„ì¶”", "ì¢‹ì•„ìš”", "ë³„ë¡œ", "ê°•ì¶”", "ê°€ê²©", "ì„±ëŠ¥", "ì˜ˆì˜ë‹¤", "ê¶ê¸ˆ"]
    for i in range(count):
        comment_text = f"ì´ ì˜ìƒ {video_id} ê´€ë ¨ ëŒ“ê¸€ì…ë‹ˆë‹¤. {keywords[i % len(keywords)]}ì— ëŒ€í•œ ì˜ê²¬ì´ì˜ˆìš”. ì •ë§ {['ì¢‹ì•„ìš”', 'ë³„ë¡œì˜ˆìš”', 'ê´œì°®ë„¤ìš”'][i % 3]}."
        data.append({
            'comment_id': f'C_{i}',
            'text': comment_text,
            'like_count': i % 20 + 1,
            'author': f'User_{i % 10}',
            'published_at': (now_kst() - timedelta(hours=i)).isoformat()
        })
    df = pd.DataFrame(data)
    return df

# ===== 2. Gemini API í˜¸ì¶œ ë¡œì§ (ì±—ë´‡ì˜ í•µì‹¬) =====

# 1) ìì—°ì–´ ì§ˆë¬¸ì„ ë¶„ì„ ëª…ë ¹ JSONìœ¼ë¡œ íŒŒì‹±í•˜ëŠ” ëª¨ë¸
def parse_user_query_to_json(user_query: str, last_url: str):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë§ˆì§€ë§‰ URLì„ ê¸°ë°˜ìœ¼ë¡œ JSON í˜•íƒœì˜ ë¶„ì„ ëª…ë ¹ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not GEMINI_API_KEY:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ íŒŒì‹±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ë¶„ì„ ëª…ë ¹ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ytccai_cloudì˜ ë¡œì§ì„ í™œìš©í•˜ê¸° ìœ„í•œ ëª…ë ¹ êµ¬ì¡°)
    schema = {
        "type": "OBJECT",
        "properties": {
            "target_url": {
                "type": "STRING",
                "description": f"ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ YouTube ì˜ìƒ URL. ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ 'last_url' ê°’ì¸ '{last_url}'ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ë¹„ì›Œë‘¡ë‹ˆë‹¤."
            },
            "analysis_type": {
                "type": "STRING",
                "description": "ë¶„ì„ ìœ í˜•: 'SUMMARY' (ì „ì²´ ìš”ì•½), 'KEYWORD_SEARCH' (í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ìš”ì•½), 'SENTIMENT' (ê¸ë¶€ì • ë¶„ì„ ìš”ì²­), 'TOPICS' (ì£¼ì œë³„ ë¶„ë¥˜ ìš”ì²­) ì¤‘ í•˜ë‚˜"
            },
            "keywords": {
                "type": "ARRAY",
                "items": {"type": "STRING"},
                "description": "ë¶„ì„ì— í•„ìš”í•œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (KEYWORD_SEARCHì˜ ê²½ìš°). ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
            }
        },
        "required": ["analysis_type"]
    }

    system_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ì ìš”ì²­ì„ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì‹œìŠ¤í…œì´ ì´í•´í•  ìˆ˜ ìˆëŠ” JSON ëª…ë ¹ì–´ë¡œ ë³€í™˜í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ 'analysis_request' ê°ì²´ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. "
        f"ë§ˆì§€ë§‰ìœ¼ë¡œ ì‚¬ìš©ëœ URLì€ '{last_url}'ì…ë‹ˆë‹¤. ë§Œì•½ ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ URLì„ ì œê³µí•˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ URLì„ 'target_url'ì— ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤. "
        "URLì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ëª…ì‹œë˜ì§€ ì•Šì•˜ë‹¤ë©´ 'target_url'ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ë‘¡ë‹ˆë‹¤. "
        "í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•˜ë©°, JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤."
    )
    
    # API í˜¸ì¶œ
    try:
        payload = {
            "contents": [{ "parts": [{ "text": user_query }] }],
            "systemInstruction": { "parts": [{ "text": system_prompt }] },
            "generationConfig": {
                "responseMimeType": "application/json",
                "responseSchema": schema
            }
        }
        
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={GEMINI_API_KEY}"
        
        # Exponential Backoff ì ìš©
        for attempt in range(3):
            response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            if response.status_code == 200:
                result = response.json()
                json_string = result["candidates"][0]["content"]["parts"][0]["text"]
                return json.loads(json_string)
            elif response.status_code == 429 and attempt < 2:
                time.sleep(2 ** attempt)  # 1ì´ˆ, 2ì´ˆ ëŒ€ê¸°
                continue
            else:
                st.error(f"íŒŒì‹± ëª¨ë¸ API í˜¸ì¶œ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
                st.json(response.json())
                return None
    except Exception as e:
        st.error(f"íŒŒì‹± ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# 2) ë¶„ì„ ê²°ê³¼ë¥¼ ìì—°ì–´ ë‹µë³€ìœ¼ë¡œ í•©ì„±í•˜ëŠ” ëª¨ë¸
def synthesize_response(original_query: str, analysis_data: str):
    """
    ë¶„ì„ ë°ì´í„°(ëŒ“ê¸€ í…ìŠ¤íŠ¸, í†µê³„ ë“±)ì™€ ì›ë˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not GEMINI_API_KEY:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    system_prompt = (
        "ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì „ë¬¸ê°€ì´ì ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì œê³µëœ ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µì°°ë ¥ ìˆê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "
        "ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ì§€ ì•Šê³ , í•µì‹¬ ìš”ì•½ê³¼ í†µê³„ë¥¼ í¬í•¨í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”. "
        "ì‚¬ìš©ìê°€ ì§€ì •í•œ URLì˜ ëŒ“ê¸€ì„ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. "
        f"ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸: '{original_query}'"
    )
    
    user_prompt = f"ë¶„ì„ëœ ë°ì´í„° ìš”ì•½:\n\n{analysis_data}\n\nì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."

    # API í˜¸ì¶œ
    try:
        payload = {
            "contents": [{ "parts": [{ "text": user_prompt }] }],
            "systemInstruction": { "parts": [{ "text": system_prompt }] },
            "config": { "temperature": 0.7 }
        }
        
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={GEMINI_API_KEY}"
        
        for attempt in range(3):
            response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            if response.status_code == 200:
                result = response.json()
                return result["candidates"][0]["content"]["parts"][0]["text"]
            elif response.status_code == 429 and attempt < 2:
                time.sleep(2 ** attempt)
                continue
            else:
                return f"âš ï¸ ë‹µë³€ í•©ì„± ëª¨ë¸ API í˜¸ì¶œ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}"

    except Exception as e:
        return f"âš ï¸ ë‹µë³€ í•©ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ===== 3. Streamlit UI ë° ì±—ë´‡ ë¡œì§ êµ¬í˜„ =====

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.last_url = ""
    st.session_state.comments_df = None
    st.session_state.messages.append({"role": "assistant", "content": 
        "ì•ˆë…•í•˜ì„¸ìš”! ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡ì…ë‹ˆë‹¤. ğŸ¤–\n\në¨¼ì € **ë¶„ì„í•˜ê³  ì‹¶ì€ YouTube ì˜ìƒ URL**ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. URL ì…ë ¥ í›„ ëŒ“ê¸€ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. (ì˜ˆ: 'ì‹ ì œí’ˆì— ëŒ€í•œ ë°˜ì‘ì´ ì–´ë•Œ?', 'ê°€ì¥ ì¢‹ì•„ìš”ë¥¼ ë§ì´ ë°›ì€ ëŒ“ê¸€ì€ ë­ì•¼?')"
    })

# --- Chat Display ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Chat Input Handler ---
if prompt := st.chat_input("YouTube URLì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„í•  ë‚´ìš©ì„ ì§ˆë¬¸í•˜ì„¸ìš”."):
    
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("AIê°€ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ë¶„ì„ì„ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            
            # 2. Gemini íŒŒì‹± (ì§ˆë¬¸ -> JSON ëª…ë ¹ì–´)
            parsed_command = parse_user_query_to_json(prompt, st.session_state.last_url)

            if parsed_command is None:
                st.error("ì§ˆë¬¸ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                st.session_state.messages.append({"role": "assistant", "content": "ì§ˆë¬¸ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."})
                safe_rerun()

            target_url = parsed_command.get("target_url")
            analysis_type = parsed_command.get("analysis_type", "SUMMARY")
            keywords = parsed_command.get("keywords", [])
            
            # URL ìœ íš¨ì„± ê²€ì‚¬ ë° ì—…ë°ì´íŠ¸
            video_id = parse_youtube_url(target_url)
            
            # 3. ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
            df = st.session_state.comments_df
            
            if video_id:
                # ìƒˆë¡œìš´ URLì´ ì…ë ¥ë˜ì—ˆê±°ë‚˜ URLì´ ë³€ê²½ëœ ê²½ìš°
                if video_id != parse_youtube_url(st.session_state.last_url):
                    st.info(f"ìƒˆë¡œìš´ URLì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤. ëŒ“ê¸€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (ID: {video_id})")
                    df = mock_fetch_comments(video_id, count=200) # ëŒ“ê¸€ 200ê°œ ê°€ìƒ ìˆ˜ì§‘
                    st.session_state.comments_df = df
                    st.session_state.last_url = target_url
                else:
                    st.info(f"ê¸°ì¡´ URL ({st.session_state.last_url})ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            elif not df:
                # URLë„ ì—†ê³  ê¸°ì¡´ ë°ì´í„°ë„ ì—†ëŠ” ê²½ìš°
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶„ì„í•  YouTube ì˜ìƒ URLì´ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                safe_rerun()
                
            if df is not None:
                # 4. ë¶„ì„ ë°ì´í„° ê°€ê³µ
                st.info(f"ì´ {len(df)}ê°œì˜ ëŒ“ê¸€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš”ì²­í•˜ì‹  '{analysis_type}' ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
                analysis_data_str = ""
                
                # ë¶„ì„ ìœ í˜•ì— ë”°ë¥¸ ë°ì´í„° ê°€ê³µ ë¡œì§ (ytccai_cloud.pyì˜ ë¡œì§ ê¸°ë°˜)
                
                if analysis_type == "KEYWORD_SEARCH" and keywords:
                    # í‚¤ì›Œë“œ ê²€ìƒ‰
                    keywords_pattern = '|'.join(re.escape(k) for k in keywords)
                    filtered_df = df[df['text'].str.contains(keywords_pattern, case=False, na=False)].copy()
                    
                    if not filtered_df.empty:
                        # ëŒ“ê¸€ 10ê°œì™€ í†µê³„ ìš”ì•½
                        top_comments = filtered_df.sort_values(by='like_count', ascending=False).head(10)
                        analysis_data_str += f"### í‚¤ì›Œë“œ '{', '.join(keywords)}' ê´€ë ¨ ëŒ“ê¸€ ({len(filtered_df)}ê°œ ë°œê²¬)\n"
                        analysis_data_str += filtered_df.describe(include='all').to_markdown() + "\n\n"
                        analysis_data_str += "#### ìƒìœ„ ëŒ“ê¸€ 10ê°œ:\n"
                        analysis_data_str += top_comments[['text', 'like_count']].to_markdown(index=False)
                    else:
                        analysis_data_str = f"í‚¤ì›Œë“œ '{', '.join(keywords)}'ì™€ ì¼ì¹˜í•˜ëŠ” ëŒ“ê¸€ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ëŒ“ê¸€ ìš”ì•½ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤."
                        analysis_type = "SUMMARY"

                if analysis_type == "SUMMARY":
                    # ì „ì²´ ìš”ì•½
                    total_comments = len(df)
                    unique_authors = df['author'].nunique()
                    
                    top_liked = df.sort_values(by='like_count', ascending=False).iloc[0]
                    
                    # ê°€ì¥ ìµœê·¼ ëŒ“ê¸€ 50ê°œë¥¼ ìš”ì•½ì— ì‚¬ìš©
                    sample_comments = df['text'].tail(50).str.cat(sep='\n---\n')
                    
                    analysis_data_str += "### ì „ì²´ ëŒ“ê¸€ ë¶„ì„ í†µê³„\n"
                    analysis_data_str += f"- ì´ ëŒ“ê¸€ ìˆ˜: {total_comments}ê°œ\n"
                    analysis_data_str += f"- ê³ ìœ  ì‘ì„±ì ìˆ˜: {unique_authors}ëª…\n"
                    analysis_data_str += f"- ìµœë‹¤ ì¢‹ì•„ìš” ëŒ“ê¸€: \"{top_liked['text'][:50]}...\" ({top_liked['like_count']}ê°œ)\n\n"
                    analysis_data_str += "#### Geminiê°€ ë¶„ì„í•  ìµœê·¼ ëŒ“ê¸€ ìƒ˜í”Œ (50ê°œ):\n"
                    analysis_data_str += sample_comments
                    
                
                # 5. Gemini ë‹µë³€ í•©ì„±
                final_response = synthesize_response(prompt, analysis_data_str)
                st.markdown(final_response)
                st.session_state.messages.append({"role": "assistant", "content": final_response})
                
            safe_rerun()

# --- ì‚¬ì´ë“œë°” ë° ë””ë²„ê¹… ì •ë³´ (ì„ íƒ ì‚¬í•­) ---
with st.sidebar:
    st.header("âš™ï¸ ì±—ë´‡ ìƒíƒœ")
    st.caption("ê°œë°œ ë° ë””ë²„ê¹… ì •ë³´")
    
    st.markdown("---")
    st.subheader("ë§ˆì§€ë§‰ ë¶„ì„ URL")
    st.code(st.session_state.last_url)

    st.subheader("ìˆ˜ì§‘ëœ ëŒ“ê¸€ ìˆ˜")
    if st.session_state.comments_df is not None:
        st.info(f"{len(st.session_state.comments_df)}ê°œ")
    else:
        st.info("ë°ì´í„° ì—†ìŒ")
        
    st.markdown("---")
    st.subheader("ë¡œì»¬ ë°ì´í„° ì •ë¦¬")
    if st.button("ğŸ—‘ï¸ ì„¸ì…˜ ì´ˆê¸°í™”", type="secondary"):
        st.session_state.clear()
        safe_rerun()
