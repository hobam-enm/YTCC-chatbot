# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ AI ìš”ì•½ ì±—ë´‡ (ìš”ì²­ ì¤€ìˆ˜: UI ê°œì„ , ì •ëŸ‰ ë¶„ì„ ì™„ì „ ì œê±°, í”„ë¡¬í”„íŠ¸/ìŠ¤í‚¤ë§ˆ ì›ë³¸ ë³µì›)
# - ì›ë³¸ íŒŒì¼ì˜ "í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ë‹¤ì¤‘ ì˜ìƒ ëŒ“ê¸€ ìˆ˜ì§‘" ë¡œì§ ë³µì›
# - ì •ëŸ‰ ë¶„ì„ ë¡œì§ (ì°¨íŠ¸, í‚¤ì›Œë“œ ë¹ˆë„, í˜•íƒœì†Œ ë¶„ì„) ì™„ì „íˆ ì œê±°
# - Gemini ìŠ¤í‚¤ë§ˆ (parse_user_query_to_schema) ì›ë³¸ ë³µì¡ êµ¬ì¡° ê·¸ëŒ€ë¡œ ë³µì›

import streamlit as st
import pandas as pd
import os, re, gc, time, json
import requests
from urllib.parse import urlparse, parse_qs
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from uuid import uuid4

# Google APIs
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ì •ëŸ‰ ë¶„ì„ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ (plotly, kiwi, stopwordsiso, circlify ë“±)ëŠ” ëª¨ë‘ ì œê±°ë¨

# ============== 0) í˜ì´ì§€/ê¸°ë³¸ ì„¤ì • ==============
st.set_page_config(page_title="ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ AI ìš”ì•½ ì±—ë´‡", layout="wide")

# UI ìŠ¤íƒ€ì¼ë§ (ì±„íŒ… ì•± ëŠë‚Œ ê°•ì¡°)
st.markdown("""
<style>
.stApp {
    background-color: #f0f2f6;
}
.stChatMessage {
    background-color: #ffffff;
    border-radius: 12px;
    padding: 15px;
    margin-bottom: 10px;
    box-shadow: 0 1px 3px rgba(0,0,0,0.05);
}
.stChatInput {
    border-top: 1px solid #e0e0e0;
    padding-top: 10px;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ AI ìš”ì•½ ì±—ë´‡")
st.markdown("**í‚¤ì›Œë“œ(ì˜ˆ: ì‹ ì œí’ˆ ë¦¬ë·°) ë˜ëŠ” URL**ì„ ì…ë ¥í•˜ê³ , ê¸°ê°„/í‚¤ì›Œë“œì™€ í•¨ê»˜ ìš”ì•½ì„ ìš”ì²­í•´ ë³´ì„¸ìš”. (ì˜ˆ: `ì‹ ì œí’ˆ ë¦¬ë·° ì˜ìƒ 1ì£¼ì¼ì¹˜ ìš”ì•½í•´ì¤˜`)")
st.markdown("---")

BASE_DIR = "/tmp"; os.makedirs(BASE_DIR, exist_ok=True)
KST = timezone(timedelta(hours=9))

def now_kst(): return datetime.now(tz=KST)
def to_iso_kst(dt: datetime) -> str:
    if dt.tzinfo is None: dt = dt.replace(tzinfo=KST)
    return dt.astimezone(KST).isoformat(timespec="seconds")
def parse_youtube_url(url: str) -> str:
    if "youtu.be" in url: return urlparse(url).path[1:]
    if "youtube.com" in url:
        query = parse_qs(urlparse(url).query)
        if 'v' in query: return query['v'][0]
    return ""
def safe_rerun():
    """Streamlitì˜ Rerunì„ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ"""
    try: st.rerun()
    except: pass


# ============== 1) API Key ê´€ë¦¬ ==============

@st.cache_resource
def _get_available_keys(key_name: str, default_fallback: list) -> list:
    """Streamlit Secretsì—ì„œ API í‚¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
    try:
        keys = st.secrets.get(key_name, default_fallback)
        if not isinstance(keys, list):
            if isinstance(keys, str) and keys: return [keys]
            raise ValueError(f"Secretsì˜ '{key_name}' ê°’ì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return [k for k in keys if k and k.startswith('AIzaSy')]
    except Exception:
        return default_fallback

# --- API Keys ë¡œë”© (Secrets/í™˜ê²½ ë³€ìˆ˜ ìš°ì„ ) ---
YT_API_KEYS = _get_available_keys("YT_API_KEYS", [""])
GEMINI_API_KEYS = _get_available_keys("GEMINI_API_KEYS", [""])

if not YT_API_KEYS or YT_API_KEYS == [""]:
    st.sidebar.error("ğŸš¨ YouTube API Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
if not GEMINI_API_KEYS or GEMINI_API_KEYS == [""]:
    st.sidebar.error("ğŸš¨ Gemini API Keyë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# ìƒíƒœ ì´ˆê¸°í™”
if "last_video_ids" not in st.session_state: st.session_state.last_video_ids = []
if "last_comments_file" not in st.session_state: st.session_state.last_comments_file = None
if "last_video_info_df" not in st.session_state: st.session_state.last_video_info_df = None 
if "last_schema" not in st.session_state: st.session_state.last_schema = None
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ğŸ¤– **ë¶„ì„í•  í‚¤ì›Œë“œ ë˜ëŠ” YouTube ì˜ìƒ URL**ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. (ì˜ˆ: 'ì‹ ì œí’ˆ ë¦¬ë·° ì˜ìƒ ëŒ“ê¸€ 1ì£¼ì¼ì¹˜ ìš”ì•½í•´ì¤˜')"}]

# ============== 2) YouTube API ë¡œì§ (ë‹¤ì¤‘ ì˜ìƒ ì²˜ë¦¬ ë¡œì§) ==============

def _get_youtube_service(key: str):
    """YouTube API ì„œë¹„ìŠ¤ ë¹Œë“œ."""
    try: 
        if not key: return None
        return build('youtube', 'v3', developerKey=key, cache_discovery=False)
    except Exception: return None

def _rotate_key(api_keys: list) -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ì¤‘ í•˜ë‚˜ë¥¼ ìˆœí™˜í•˜ì—¬ ë°˜í™˜."""
    if not api_keys: return None
    key = api_keys.pop(0)
    api_keys.append(key)
    return key

def search_videos(query: str, max_videos: int = 5, published_after: datetime = None) -> (list, pd.DataFrame or None):
    """YouTube ê²€ìƒ‰ APIë¥¼ í†µí•´ ì˜ìƒì„ ê²€ìƒ‰í•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    api_keys = YT_API_KEYS[:]
    video_list = []
    
    # ISO 8601 í˜•ì‹ (UTC)
    published_after_utc = published_after.astimezone(timezone.utc).isoformat("T") + "Z" if published_after else None
    
    for _ in range(len(api_keys)):
        key = _rotate_key(api_keys)
        if not key: continue
        try:
            youtube = _get_youtube_service(key)
            if not youtube: continue
            
            response = youtube.search().list(
                q=query,
                part="snippet",
                type="video",
                maxResults=max_videos,
                publishedAfter=published_after_utc,
                order="relevance"
            ).execute()
            
            for item in response.get("items", []):
                video_id = item["id"]["videoId"]
                published_at_utc = datetime.fromisoformat(item["snippet"]["publishedAt"].replace('Z', '+00:00'))
                
                video_list.append({
                    "video_id": video_id,
                    "title": item["snippet"]["title"],
                    "channelTitle": item["snippet"]["channelTitle"],
                    "published_at_kst": to_iso_kst(published_at_utc.astimezone(KST)),
                    "commentCount": 0 
                })
            
            if video_list:
                df_videos = pd.DataFrame(video_list)
                return df_videos['video_id'].tolist(), df_videos
            
        except HttpError as e:
            if 'quotaExceeded' in str(e) or 'keyInvalid' in str(e):
                st.warning(f"YouTube API í‚¤ í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ë¬´íš¨: {key[:10]}...")
                continue
            st.error(f"ë¹„ë””ì˜¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            break
        except Exception as e:
            st.error(f"ë¹„ë””ì˜¤ ê²€ìƒ‰ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
            break
    return [], None

def fetch_single_video_comments(video_id: str, max_results: int, api_keys: list, result_queue: list):
    """ë‹¨ì¼ ë¹„ë””ì˜¤ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ê²°ê³¼ë¥¼ íì— ì¶”ê°€í•©ë‹ˆë‹¤. (Thread worker)"""
    
    total_comments = 0
    next_page_token = None
    
    try:
        # API í‚¤ ìˆœí™˜ ë° YouTube ì„œë¹„ìŠ¤ íšë“ (í‚¤ ì˜¤ë¥˜ì‹œ ì¬ì‹œë„ 3íšŒ)
        for attempt in range(len(api_keys)):
            key = api_keys[(attempt + 1) % len(api_keys)] 
            youtube = _get_youtube_service(key)
            if not youtube: continue
            
            comments_data = []
            
            # ìˆ˜ì§‘ ë£¨í”„ (í˜ì´ì§€ ìµœëŒ€ 100ê°œ * 50 = 5000ê°œ ì œí•œ)
            for _ in range(100): 
                request = youtube.commentThreads().list(
                    part="snippet",
                    videoId=video_id,
                    maxResults=50,
                    pageToken=next_page_token,
                    order="time"
                )
                response = request.execute()
                
                # ëŒ“ê¸€ ì“°ê¸°
                for item in response.get("items", []):
                    top_level_comment = item["snippet"]["topLevelComment"]["snippet"]
                    
                    comments_data.append({
                        "comment_id": item["snippet"]["topLevelComment"]["id"],
                        "author": top_level_comment["authorDisplayName"],
                        "published_at": top_level_comment["publishedAt"],
                        "text": top_level_comment["textDisplay"].replace('\n', ' ').replace('\r', ' '), 
                        "like_count": top_level_comment["likeCount"],
                        "video_id": video_id
                    })
                    total_comments += 1

                next_page_token = response.get('nextPageToken')
                if not next_page_token or total_comments >= max_results:
                    break
            
            result_queue.append(pd.DataFrame(comments_data))
            return 
            
        # ëª¨ë“  API í‚¤ ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
        result_queue.append(None)
    
    except HttpError as e:
        if 'commentsDisabled' in str(e):
            result_queue.append(f"Comments Disabled:{video_id}")
        elif 'quotaExceeded' in str(e) or 'keyInvalid' in str(e):
             result_queue.append(f"Quota Error:{video_id}")
        else:
             result_queue.append(f"HTTP Error:{video_id}:{e}")
    except Exception:
        result_queue.append(f"Unknown Error:{video_id}")

def concurrent_fetch_comments(video_ids: list, total_comments_limit=5000) -> (pd.DataFrame or None, str):
    """ì—¬ëŸ¬ ì˜ìƒ IDì— ëŒ€í•´ ë™ì‹œì— ëŒ“ê¸€ì„ ìˆ˜ì§‘í•˜ê³  í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤."""
    
    if not video_ids: return None, "ìˆ˜ì§‘í•  ì˜ìƒ IDê°€ ì—†ìŠµë‹ˆë‹¤."

    api_keys = YT_API_KEYS[:]
    result_queue = []
    
    with ThreadPoolExecutor(max_workers=min(len(video_ids), 5)) as executor:
        futures = [executor.submit(fetch_single_video_comments, vid, total_comments_limit // len(video_ids), api_keys, result_queue) for vid in video_ids]
        
        for future in as_completed(futures):
            try:
                future.result() 
            except Exception as e:
                print(f"ëŒ“ê¸€ ìˆ˜ì§‘ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")

    successful_dfs = [df for df in result_queue if isinstance(df, pd.DataFrame) and not df.empty]
    error_messages = [msg for msg in result_queue if isinstance(msg, str)]

    if not successful_dfs:
        return None, "ëª¨ë“  ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ëŒ“ê¸€ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    df_combined = pd.concat(successful_dfs, ignore_index=True)
    
    filename = os.path.join(BASE_DIR, f"comments_combined_{now_kst().strftime('%Y%m%d%H%M%S')}.csv")
    df_combined.to_csv(filename, index=False, encoding='utf-8-sig')

    return df_combined, filename

# ============== 3) Gemini ë¡œì§ (ì›ë³¸ ìŠ¤í‚¤ë§ˆ/í”„ë¡¬í”„íŠ¸ ë³µì›) ==============

def _rotate_gemini_key():
    """Gemini API í‚¤ ìˆœí™˜."""
    return _rotate_key(GEMINI_API_KEYS)

def parse_user_query_to_schema(user_query: str, last_video_ids: list) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìœ íŠœë¸Œ ë¶„ì„ì— í•„ìš”í•œ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ì›ë³¸ íŒŒì¼ì˜ ë³µì¡í•œ ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ê·¸ëŒ€ë¡œ ìœ ì§€)
    """
    key = _rotate_gemini_key()
    if not key:
        return {"error": "Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # === ìš”ì²­ëŒ€ë¡œ ì›ë³¸ íŒŒì¼ì˜ ë³µì¡í•œ ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ê·¸ëŒ€ë¡œ ë³µì› ===
    schema = {
        "type": "OBJECT",
        "properties": {
            "search_term": {
                "type": "STRING",
                "description": "ì‚¬ìš©ìê°€ ê²€ìƒ‰ì„ ìš”ì²­í•œ ê²½ìš°ì˜ í‚¤ì›Œë“œ (ì˜ˆ: 'ì‹ ì œí’ˆ ë¦¬ë·°', 'ê°¤ëŸ­ì‹œ S24'). URLì´ ì•„ë‹Œ ì¼ë°˜ í‚¤ì›Œë“œì…ë‹ˆë‹¤. URLì´ í¬í•¨ëœ ê²½ìš° ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
            },
            "video_id": {
                "type": "STRING",
                "description": "ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ë‹¨ì¼ ìœ íŠœë¸Œ ì˜ìƒ ID (URLì—ì„œ ì¶”ì¶œëœ ID). ë‹¨ì¼ ì˜ìƒ ë¶„ì„ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
            },
            "analysis_type": {
                "type": "STRING",
                "description": "'FULL_ANALYSIS' (ìƒˆë¡œìš´ ê²€ìƒ‰/ê¸°ê°„ ìš”ì²­), 'CHAT_FOLLOW_UP' (ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸), 'SIMPLE_QA' (ì¼ë°˜ ì§ˆë¬¸) ì¤‘ í•˜ë‚˜."
            },
            "start_iso": {
                "type": "STRING",
                "description": "ë¶„ì„ ì‹œì‘ ë‚ ì§œ (ISO 8601 í˜•ì‹). ì˜ˆ: '2024-01-01T00:00:00+09:00'"
            },
            "end_iso": {
                "type": "STRING",
                "description": "ë¶„ì„ ì¢…ë£Œ ë‚ ì§œ (ISO 8601 í˜•ì‹). ì˜ˆ: '2024-03-31T23:59:59+09:00'"
            },
            "keywords": {
                "type": "ARRAY",
                "items": {"type": "STRING"},
                "description": "ëŒ“ê¸€ ë°ì´í„° ë‚´ì—ì„œ íŠ¹ë³„íˆ í•„í„°ë§/ìš”ì•½í•˜ë ¤ëŠ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: 'ê°€ê²©', 'ì„±ëŠ¥', 'ë²„ê·¸')."
            },
            "filter_type": {
                "type": "STRING",
                "description": "ëŒ“ê¸€ í•„í„°ë§ ê¸°ì¤€ ('ALL', 'RELEVANCE', 'LIKES' ì¤‘ í•˜ë‚˜). ê¸°ë³¸ê°’ì€ 'ALL'."
            },
            "entities": {
                "type": "ARRAY",
                "items": {"type": "STRING"},
                "description": "ëŒ“ê¸€ ë°ì´í„° ë‚´ì—ì„œ ì–¸ê¸‰ ë¹ˆë„/ì •ì„œ ë¶„ì„ì— ì‚¬ìš©í•  ëª…ì‚¬/ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: 'ê°€ê²©', 'ë°°í„°ë¦¬ ìˆ˜ëª…')."
            }
        },
        "required": ["analysis_type"]
    }
    # ==============================================================
    
    # ê¸°ë³¸ê°’ ì„¤ì •: ì§€ë‚œ 30ì¼
    today = now_kst()
    a_month_ago = today - timedelta(days=30)
    
    current_schema = {
        "search_term": "",
        "video_id": parse_youtube_url(user_query) if ('youtube.com' in user_query or 'youtu.be' in user_query) else "",
        "start_iso": to_iso_kst(a_month_ago.replace(hour=0, minute=0, second=0, microsecond=0)),
        "end_iso": to_iso_kst(today.replace(hour=23, minute=59, second=59, microsecond=0)),
        "keywords": [],
        "filter_type": "ALL",
        "entities": []
    }
    
    # === ìš”ì²­ëŒ€ë¡œ ì›ë³¸ íŒŒì¼ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ê·¸ëŒ€ë¡œ ë³µì› ===
    last_ids_str = ', '.join(last_video_ids) if last_video_ids else 'N/A'

    system_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ì ìš”ì²­ì„ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì‹œìŠ¤í…œì´ ì´í•´í•  ìˆ˜ ìˆëŠ” JSON ëª…ë ¹ì–´ë¡œ ë³€í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, URLì´ ìˆìœ¼ë©´ 'video_id'ì—, URLì´ ì—†ìœ¼ë©´ ì¼ë°˜ í‚¤ì›Œë“œë¥¼ 'search_term'ì— ì¶”ì¶œí•˜ì„¸ìš”. "
        f"ë§ˆì§€ë§‰ìœ¼ë¡œ ë¶„ì„í•œ ì˜ìƒ IDë“¤(ë³µìˆ˜ ê°€ëŠ¥)ì€ '{last_ids_str}'ì…ë‹ˆë‹¤. í˜„ì¬ ë‚ ì§œëŠ” {today.strftime('%Yë…„ %mì›” %dì¼')}ì…ë‹ˆë‹¤. "
        "ìƒˆë¡œìš´ ì˜ìƒ/ê²€ìƒ‰ ìš”ì²­ì´ë©´ 'FULL_ANALYSIS'ë¡œ, ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸ì´ë©´ 'CHAT_FOLLOW_UP', ì¼ë°˜ ì§ˆë¬¸ì€ 'SIMPLE_QA'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. "
        "ê¸°ê°„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì§€ë‚œ 30ì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 'filter_type'ì´ ì–¸ê¸‰ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ 'ALL'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤."
    )
    # ==============================================================
    
    try:
        payload = {
            "contents": [{ "parts": [{ "text": user_query }] }],
            "systemInstruction": { "parts": [{ "text": system_prompt }] },
            "generationConfig": {
                "responseMimeType": "application/json",
                "responseSchema": schema
            }
        }
        
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={key}"
        
        for attempt in range(3):
            response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            if response.status_code == 200:
                json_string = response.json()["candidates"][0]["content"]["parts"][0]["text"]
                parsed = json.loads(json_string)
                
                # ë¹ˆ ë¬¸ìì—´ì„ Noneìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê¸°ì¡´ current_schemaì˜ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
                final_schema = {**current_schema, **parsed}
                return final_schema

            elif response.status_code == 429 and attempt < 2:
                time.sleep(2 ** attempt)
                continue
            else:
                return {"error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}"}
    except Exception as e:
        return {"error": str(e)}

def synthesize_response(query: str, analysis_data: str, video_info_df: pd.DataFrame or None, schema: dict) -> str:
    """ë¶„ì„ ë°ì´í„°ì™€ ì›ë˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (AI ìš”ì•½)"""
    key = _rotate_gemini_key()
    if not key: return "Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤."

    system_prompt = (
        "ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì „ë¬¸ê°€ì´ì ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ì œê³µëœ 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°'ì™€ 'ë¹„ë””ì˜¤ ì •ë³´(DF)'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í†µì°°ë ¥ ìˆê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "
        "ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ì§€ ì•Šê³ , í•µì‹¬ ìš”ì•½ê³¼ í†µê³„ë¥¼ í¬í•¨í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”. "
        "ë§Œì•½ 'ë¶„ì„ ê²°ê³¼ ë°ì´í„°'ê°€ ë¹„ì–´ ìˆë‹¤ë©´, ë¹„ë””ì˜¤ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    )
    
    # ë¹„ë””ì˜¤ ì •ë³´ ìš”ì•½ (ì—¬ëŸ¬ ê°œì˜ ë¹„ë””ì˜¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìì—´ë¡œ ë³€í™˜)
    if video_info_df is not None and not video_info_df.empty:
        video_info_str = "ë¶„ì„ ëŒ€ìƒ ì˜ìƒ ëª©ë¡:\n"
        for _, row in video_info_df.iterrows():
            # video_idë¥¼ í¬í•¨í•˜ì—¬ ë¶„ì„ëœ ì˜ìƒ ëª©ë¡ ì •ë³´ ì œê³µ
            video_info_str += f"- ì œëª©: {row['title']} (ì±„ë„: {row['channelTitle']}) [ID: {row['video_id']}]\n"
    else:
        video_info_str = "N/A"
    
    user_prompt = (
        f"ì‚¬ìš©ìì˜ ì§ˆë¬¸: '{query}'\n"
        f"ë¶„ì„ ìš”ì²­ ìŠ¤í‚¤ë§ˆ: {json.dumps(schema, ensure_ascii=False)}\n"
        f"ë¹„ë””ì˜¤ ê¸°ë³¸ ì •ë³´:\n{video_info_str}\n"
        f"ëŒ“ê¸€ ë¶„ì„ ê²°ê³¼ ë°ì´í„°:\n{analysis_data}\n\n"
        "ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ëŒ“ê¸€ ë°ì´í„°ë¥¼ **ìš”ì•½**í•˜ê³ , í‚¤ì›Œë“œë‚˜ ê¸°ê°„ì´ ì§€ì •ë˜ì—ˆë‹¤ë©´ ê·¸ ê²°ê³¼ë¥¼ **í•µì‹¬ ë¬¸ì¥**ìœ¼ë¡œ ê°•ì¡°í•˜ì„¸ìš”. ë‹µë³€ ì‹œ, ë¶„ì„ëœ ì˜ìƒì˜ **ì œëª©**ê³¼ **ìˆ˜**ë¥¼ ì–¸ê¸‰í•˜ì—¬ ê²°ê³¼ë¥¼ êµ¬ì²´í™”í•´ì£¼ì„¸ìš”."
    )

    try:
        payload = {
            "contents": [{ "parts": [{ "text": user_prompt }] }],
            "systemInstruction": { "parts": [{ "text": system_prompt }] }
        }
        
        apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key={key}"
        
        for attempt in range(3):
            response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            if response.status_code == 200:
                return response.json()["candidates"][0]["content"]["parts"][0]["text"]
            elif response.status_code == 429 and attempt < 2:
                time.sleep(2 ** attempt)
                continue
            else:
                return f"âš ï¸ ë‹µë³€ í•©ì„± ëª¨ë¸ API í˜¸ì¶œ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}"
    except Exception as e:
        return f"âš ï¸ ë‹µë³€ í•©ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ============== 4) ë°ì´í„° í•„í„°ë§ ë° ìš”ì•½ ë¡œì§ ==============

def get_filtered_comments_df(comments_file: str, schema: dict) -> pd.DataFrame:
    """ëŒ“ê¸€ CSV íŒŒì¼ì„ ì½ê³ , ê¸°ê°„ ë° í‚¤ì›Œë“œì— ë”°ë¼ í•„í„°ë§í•©ë‹ˆë‹¤."""
    if not comments_file or not os.path.exists(comments_file):
        return None
    
    df_list = []
    try:
        dtype_map = {'comment_id': str, 'author': str, 'published_at': str, 'text': str, 'like_count': 'Int64', 'video_id': str}
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ chunksize ì‚¬ìš©
        for chunk in pd.read_csv(comments_file, chunksize=100000, encoding='utf-8-sig', dtype=dtype_map):
            df_list.append(chunk)
        df = pd.concat(df_list, ignore_index=True)
    except Exception as e:
        st.error(f"CSV íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None
    
    if df.empty: return df

    # 1. ê¸°ê°„ í•„í„°ë§
    try:
        start_dt = datetime.fromisoformat(schema['start_iso'])
        end_dt = datetime.fromisoformat(schema['end_iso'])
    except ValueError:
        start_dt = now_kst() - timedelta(days=30)
        end_dt = now_kst()
    
    df['published_at'] = pd.to_datetime(df['published_at'], utc=True)
    df['published_at_kst'] = df['published_at'].dt.tz_convert(KST)
    
    df_filtered = df[(df['published_at_kst'] >= start_dt) & (df['published_at_kst'] <= end_dt)].copy()
    
    # 2. í‚¤ì›Œë“œ í•„í„°ë§ (ëŒ“ê¸€ ë‚´ìš© ê¸°ì¤€)
    keywords = [k for k in schema.get('keywords', []) if k]
    if keywords:
        pattern = '|'.join(re.escape(k) for k in keywords)
        df_filtered = df_filtered[df_filtered['text'].fillna('').str.contains(pattern, case=False, na=False)].copy()

    # 'filter_type'ì€ ì •ëŸ‰ ë¶„ì„ì— ì“°ì´ëŠ” í•„í„°ì˜€ìœ¼ë¯€ë¡œ, í˜„ì¬ëŠ” ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ëŠ” ìš©ë„ë¡œë§Œ ë‚¨ê¸°ê³  ë‹¤ë¥¸ ë¡œì§ì€ ì œê±°
    filter_type = schema.get('filter_type', 'ALL')
    if filter_type == 'LIKES':
         # ì¢‹ì•„ìš”ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 5000ê°œë§Œ í•„í„°ë§ (ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ ì´ˆê³¼ ë°©ì§€ ë° ê´€ë ¨ì„± ë†’ì€ ëŒ“ê¸€ ì„ íƒ)
         df_filtered = df_filtered.sort_values(by='like_count', ascending=False).head(5000).reset_index(drop=True)

    return df_filtered.reset_index(drop=True)

def generate_analysis_summary(df: pd.DataFrame, schema: dict) -> str:
    """í•„í„°ë§ëœ DataFrameì„ ê¸°ë°˜ìœ¼ë¡œ AIì—ê²Œ ì „ë‹¬í•  ìµœì†Œí•œì˜ í†µê³„ ìš”ì•½ ë° ëŒ“ê¸€ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì •ëŸ‰ ë¶„ì„ ì½”ë“œ(í˜•íƒœì†Œ ë¶„ì„, í‚¤ì›Œë“œ ë¹ˆë„ ë“±)ëŠ” ëª¨ë‘ ì œê±°í•˜ê³ , ì˜¤ì§ AI ìš”ì•½ì— í•„ìš”í•œ ì›ì‹œ ë°ì´í„°ë§Œ ì œê³µ

    if df is None or df.empty:
        return "í•„í„°ë§ëœ ëŒ“ê¸€ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê¸°ê°„ ë˜ëŠ” í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜)"
    
    summary = f"**[í•„í„°ë§ í†µê³„ ìš”ì•½]**\n"
    summary += f"- ìµœì¢… ë¶„ì„ ëŒ€ìƒ ëŒ“ê¸€ ìˆ˜: {len(df):,}ê°œ\n"
    summary += f"- ê³ ìœ  ì‘ì„±ì ìˆ˜: {df['author'].nunique():,}ëª…\n"
    
    # ìµœë‹¤ ì¢‹ì•„ìš” ëŒ“ê¸€
    top_liked = df.sort_values(by='like_count', ascending=False).head(1)
    if not top_liked.empty:
        summary += f"- ìµœë‹¤ ì¢‹ì•„ìš” ëŒ“ê¸€ ({top_liked['like_count'].iloc[0]}ê°œ, ì‘ì„±ì: {top_liked['author'].iloc[0]}): \"{top_liked['text'].iloc[0][:150].replace('\n', ' ')}...\"\n"

    # ëŒ“ê¸€ ìƒ˜í”Œ (AI ë‹µë³€ ë§¥ë½ ìœ ì§€ë¥¼ ìœ„í•´ ëœë¤ 20ê°œ ì„ íƒ)
    if len(df) > 20:
        sample_df = df.sample(n=20)
    else:
        sample_df = df
        
    sample_comments = sample_df['text'].str.cat(sep='\n---END---\n').replace('\n', ' ')
    summary += "\n**[ëŒ“ê¸€ ìƒ˜í”Œ (ì´ 20ê°œ)]**\n"
    summary += sample_comments
    
    return summary

# ============== 5) ë©”ì¸ ì±—ë´‡ ë£¨í”„ ==============

# --- Chat Display ---
chat_container = st.container(height=600, border=False) # ì±„íŒ… ì˜ì—­ì„ ê³ ì • ë†’ì´ ì»¨í…Œì´ë„ˆë¡œ ì„¤ì •

with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- Chat Input Handler ---
if prompt := st.chat_input("YouTube URL ë˜ëŠ” ë¶„ì„ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    
    # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì»¨í…Œì´ë„ˆë¥¼ ë‹¤ì‹œ ë Œë”ë§í•˜ì—¬ ìƒˆ ë©”ì‹œì§€ë¥¼ í‘œì‹œ
    with chat_container:
        with st.chat_message("user"):
            st.markdown(prompt)

    with st.chat_message("assistant"):
        
        # 2. Gemini íŒŒì‹± (ì§ˆë¬¸ -> Schema ëª…ë ¹ì–´)
        with st.spinner("AIê°€ ì§ˆë¬¸ì„ ì´í•´í•˜ê³  ë¶„ì„ ìŠ¤í‚¤ë§ˆë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            parsed_schema = parse_user_query_to_schema(prompt, st.session_state.last_video_ids)
            st.session_state.last_schema = parsed_schema # ìŠ¤í‚¤ë§ˆ ì €ì¥ (ì‚¬ì´ë“œë°” ì¶œë ¥ìš©)

        if parsed_schema.get("error"):
            response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {parsed_schema['error']}"
            st.error(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
            safe_rerun()
            
        video_id = parsed_schema.get("video_id", "")
        search_term = parsed_schema.get("search_term", "")
        analysis_type = parsed_schema.get("analysis_type", "SIMPLE_QA")
        
        df_filtered = None
        df_videos = st.session_state.last_video_info_df # ê¸°ì¡´ ì •ë³´ ì‚¬ìš© ì‹œë„
        final_response = "ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤." # ì´ˆê¸°í™”

        if analysis_type == "SIMPLE_QA":
            # ì¼ë°˜ QAë§Œ ìš”ì²­ë°›ì€ ê²½ìš° (ë°ì´í„° ìˆ˜ì§‘ ë¶ˆí•„ìš”)
            response = synthesize_response(prompt, "N/A", df_videos, parsed_schema)
            st.markdown(response)
            final_response = response
        
        elif analysis_type == "FULL_ANALYSIS":
            
            video_ids_to_fetch = []
            
            # --- 3-1. ë¹„ë””ì˜¤ ID(ë“¤) í™•ì • ---
            if video_id:
                # ë‹¨ì¼ URLì´ ì…ë ¥ëœ ê²½ìš°
                video_ids_to_fetch = [video_id]
                with st.spinner(f"ë‹¨ì¼ ì˜ìƒ ID({video_id}) ì •ë³´ ë¡œë“œ ì¤‘..."):
                    single_video_info = search_videos(f"video id {video_id}", max_videos=1)[1] 
                    if single_video_info is not None:
                        df_videos = single_video_info
                    else:
                        response = "ë‹¨ì¼ ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ID ë˜ëŠ” API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                        st.error(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        safe_rerun()

            elif search_term:
                # ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì…ë ¥ëœ ê²½ìš°
                start_dt = datetime.fromisoformat(parsed_schema['start_iso'])
                with st.spinner(f"í‚¤ì›Œë“œ '{search_term}'ë¡œ ì˜ìƒ ê²€ìƒ‰ ë° ì •ë³´ ë¡œë“œ ì¤‘..."):
                    video_ids_to_fetch, df_videos = search_videos(search_term, published_after=start_dt)
                    if not video_ids_to_fetch:
                        response = f"í‚¤ì›Œë“œ '{search_term}'ì— ëŒ€í•œ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        st.warning(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        safe_rerun()
            else:
                response = "ë¶„ì„í•  ì˜ìƒ URL ë˜ëŠ” ê²€ìƒ‰í•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                st.warning(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                safe_rerun()

            # --- 3-2. ëŒ“ê¸€ ìˆ˜ì§‘ ë° ìƒíƒœ ì €ì¥ ---
            if video_ids_to_fetch:
                with st.spinner(f"{len(video_ids_to_fetch)}ê°œ ì˜ìƒì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. (ì•½ {len(video_ids_to_fetch)*500}ê°œ ëª©í‘œ)..."):
                    df_combined, comments_file = concurrent_fetch_comments(video_ids_to_fetch)

                    if df_combined is None:
                        response = f"ëŒ“ê¸€ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í• ë‹¹ëŸ‰ ë˜ëŠ” ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”. ì˜¤ë¥˜: {comments_file}"
                        st.error(response)
                        st.session_state.messages.append({"role": "assistant", "content": response})
                        safe_rerun()
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    st.session_state.last_video_ids = video_ids_to_fetch
                    st.session_state.last_comments_file = comments_file
                    st.session_state.last_video_info_df = df_videos # <-- ë‹¤ì¤‘/ë‹¨ì¼ ì˜ìƒ DF ëª¨ë‘ ì €ì¥

                # --- 3-3. ë°ì´í„° í•„í„°ë§ ë° ìš”ì•½ ë°ì´í„° ìƒì„± ---
                with st.spinner("ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  AI ìš”ì•½ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    df_filtered = get_filtered_comments_df(comments_file, parsed_schema)
                    analysis_data = generate_analysis_summary(df_filtered, parsed_schema)
                
                # --- 3-4. AI ë‹µë³€ í•©ì„± ---
                with st.spinner("AIê°€ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    final_response = synthesize_response(prompt, analysis_data, df_videos, parsed_schema)
                    st.markdown(final_response)

        
        elif analysis_type == "CHAT_FOLLOW_UP":
            
            # --- 3-1. ê¸°ì¡´ ë°ì´í„° ì¬í™œìš© ë° í•„í„°ë§ ---
            if not st.session_state.last_comments_file or not os.path.exists(st.session_state.last_comments_file):
                response = "ì´ì „ì— ë¶„ì„ëœ ì˜ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ë˜ëŠ” URLê³¼ í•¨ê»˜ ë¶„ì„ ìš”ì²­ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”."
                st.warning(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                safe_rerun()
                
            comments_file = st.session_state.last_comments_file
            
            with st.spinner("ê¸°ì¡´ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  ì¶”ê°€ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                df_filtered = get_filtered_comments_df(comments_file, parsed_schema)
                analysis_data = generate_analysis_summary(df_filtered, parsed_schema)

            # --- 3-2. AI ë‹µë³€ í•©ì„± ---
            with st.spinner("AIê°€ ì¶”ê°€ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                final_response = synthesize_response(prompt, analysis_data, df_videos, parsed_schema)
                st.markdown(final_response)
        
    st.session_state.messages.append({"role": "assistant", "content": final_response})
    safe_rerun()

# ============== 6) ì‚¬ì´ë“œë°” ë Œë”ë§ (UI/ë‹¤ìš´ë¡œë“œ) ==============

with st.sidebar:
    st.header("âš™ï¸ ë¶„ì„ ìƒíƒœ ë° ë„êµ¬")
    
    if st.session_state.last_video_info_df is not None and not st.session_state.last_video_info_df.empty:
        df = st.session_state.last_video_info_df
        st.subheader(f"ë§ˆì§€ë§‰ ë¶„ì„ ì˜ìƒ ({len(df)}ê°œ)")
        # 5ê°œê¹Œì§€ë§Œ ë³´ì—¬ì¤Œ
        info_list = "\n".join([f"- **{row['title']}** (ì±„ë„: {row['channelTitle']})" for _, row in df.head(5).iterrows()])
        st.info(info_list)
        if len(df) > 5:
             st.caption(f"ì™¸ {len(df) - 5}ê°œ ì˜ìƒ")
    else:
        st.subheader("ë¶„ì„ ì˜ìƒ ëŒ€ê¸° ì¤‘")
        
    st.markdown("---")
    
    st.subheader("ì ìš© í•„í„° ì •ë³´")
    if st.session_state.last_schema:
        s = st.session_state.last_schema
        st.caption(f"ê²€ìƒ‰ì–´: `{s.get('search_term') or 'N/A'}` / ID: `{s.get('video_id') or 'N/A'}`")
        st.markdown(f"- **ê¸°ê°„:** `{s.get('start_iso', 'N/A').split('T')[0]} ~ {s.get('end_iso', 'N/A').split('T')[0]}`")
        keywords = s.get('keywords', [])
        entities = s.get('entities', [])
        st.markdown(f"- **ëŒ“ê¸€ í‚¤ì›Œë“œ:** {', '.join(keywords) or '(ì „ì²´)'}")
        st.markdown(f"- **ë¶„ì„ ì—”í‹°í‹°:** {', '.join(entities) or '(ì—†ìŒ)'}")
        st.markdown(f"- **í•„í„° ìœ í˜•:** `{s.get('filter_type', 'ALL')}`")
        
    st.markdown("---")
    
    st.subheader("ë‹¤ìš´ë¡œë“œ")
    # ì „ì²´ ëŒ“ê¸€ CSV ë‹¤ìš´ë¡œë“œ
    if st.session_state.last_comments_file and os.path.exists(st.session_state.last_comments_file):
        try:
            with open(st.session_state.last_comments_file, "rb") as f:
                st.download_button(
                    "â¬‡ï¸ ìˆ˜ì§‘ëœ ëŒ“ê¸€ CSV ë‹¤ìš´ë¡œë“œ",
                    data=f.read(),
                    file_name=f"comments_full_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
        except Exception:
             st.caption("ëŒ“ê¸€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
    else:
        st.caption("ë¶„ì„ëœ ëŒ“ê¸€ íŒŒì¼ ì—†ìŒ")

    # ì˜ìƒ ëª©ë¡ CSV ë‹¤ìš´ë¡œë“œ
    if st.session_state.last_video_info_df is not None and not st.session_state.last_video_info_df.empty:
        csv_videos = st.session_state.last_video_info_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button(
            "â¬‡ï¸ ë¶„ì„ ì˜ìƒ ëª©ë¡ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_videos,
            file_name=f"videos_{len(st.session_state.last_video_info_df)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    else:
        st.caption("ë¶„ì„ ì˜ìƒ ëª©ë¡ íŒŒì¼ ì—†ìŒ")
        
    st.markdown("---")
    
    # ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”", type="secondary"):
        st.session_state.clear()
        safe_rerun()
