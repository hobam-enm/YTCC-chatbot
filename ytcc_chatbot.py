# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„ê¸° â€” ìˆœìˆ˜ ì±—ë´‡ ëª¨ë“œ (ì„¸ì…˜ ê´€ë¦¬ ê¸°ëŠ¥ ìµœì¢…)

import streamlit as st
import pandas as pd
import os, re, gc, time, json, base64, requests
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from uuid import uuid4
import io

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
from streamlit.components.v1 import html as st_html

# -------------------- í˜ì´ì§€/ì „ì—­ --------------------
st.set_page_config(page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„: ì±—ë´‡", layout="wide", initial_sidebar_state="expanded")

st.markdown("""
<style>
/* Streamlit ë©”ì¸ ì»¨í…Œì´ë„ˆ íŒ¨ë”© ìµœì†Œí™” */
.main .block-container {
    padding-top: 2rem;
    padding-right: 1rem;
    padding-left: 1rem;
    padding-bottom: 5rem;
}
[data-testid="stSidebarContent"] {
    padding-top: 1.5rem;
}
header {visibility: hidden;}
footer {visibility: hidden;}
#MainMenu {visibility: hidden;}

/* --- ì‚¬ì´ë“œë°” ë„ˆë¹„ ê³ ì • --- */
[data-testid="stSidebar"] {
    width: 350px !important;
    min-width: 350px !important;
    max-width: 350px !important;
}
[data-testid="stSidebar"] + div[class*="resizer"] {
    display: none;
}
/* --- --- */

/* AI ë‹µë³€ í°íŠ¸ í¬ê¸° ì¡°ì • */
[data-testid="stChatMessage"]:has(span[data-testid="chat-avatar-assistant"]) p,
[data-testid="stChatMessage"]:has(span[data-testid="chat-avatar-assistant"]) li {
    font-size: 0.8rem;
}
/* ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í…ìŠ¤íŠ¸ ë§í¬ì²˜ëŸ¼ ë³´ì´ê²Œ ìŠ¤íƒ€ì¼ë§ */
.stDownloadButton button {
    background-color: transparent;
    color: #1c83e1;
    border: none;
    padding: 0;
    text-decoration: underline;
    font-size: 10px;
    font-weight: normal;
}
.stDownloadButton button:hover {
    color: #0b5cab;
}
/* ì„¸ì…˜ ëª©ë¡ ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.session-list .stButton button {
    font-size: 0.9rem;
    text-align: left;
    font-weight: normal;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
    display: block;
}
/* ìƒˆ ì±„íŒ… ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
.new-chat-btn button {
    background-color: #e8f0fe;
    color: #1967d2;
    border: 1px solid #d2e3fc !important;
}
.new-chat-btn button:hover {
    background-color: #d2e3fc;
    color: #185abc;
    border: 1px solid #c2d8f8 !important;
}
</style>
""", unsafe_allow_html=True)

# --- ê²½ë¡œ ë° GitHub ì„¤ì • ---
BASE_DIR = "/tmp"
SESS_DIR = os.path.join(BASE_DIR, "sessions")
os.makedirs(SESS_DIR, exist_ok=True)

GITHUB_TOKEN = st.secrets.get("GITHUB_TOKEN", "")
GITHUB_REPO = st.secrets.get("GITHUB_REPO", "")
GITHUB_BRANCH = st.secrets.get("GITHUB_BRANCH", "main")

KST = timezone(timedelta(hours=9))
def now_kst(): return datetime.now(tz=KST)
def to_iso_kst(dt: datetime) -> str:
    if dt.tzinfo is None: dt = dt.replace(tzinfo=KST)
    return dt.astimezone(KST).isoformat(timespec="seconds")
def kst_to_rfc3339_utc(dt_kst: datetime) -> str:
    if dt_kst.tzinfo is None: dt_kst = dt_kst.replace(tzinfo=KST)
    return dt_kst.astimezone(timezone.utc).isoformat().replace("+00:00","Z")

# -------------------- í‚¤/ìƒìˆ˜ --------------------
_YT_FALLBACK, _GEM_FALLBACK = [], []
YT_API_KEYS       = list(st.secrets.get("YT_API_KEYS", [])) or _YT_FALLBACK
GEMINI_API_KEYS   = list(st.secrets.get("GEMINI_API_KEYS", [])) or _GEM_FALLBACK
GEMINI_MODEL      = "gemini-2.5-flash-lite"
GEMINI_TIMEOUT    = 120
GEMINI_MAX_TOKENS = 2048
MAX_TOTAL_COMMENTS   = 120_000
MAX_COMMENTS_PER_VID = 4_000

# -------------------- ìƒíƒœ --------------------
def ensure_state():
    defaults = {"chat":[], "last_schema":None, "last_csv":"", "last_df":None, "sample_text":"", "loaded_session_name": None}
    for k, v in defaults.items():
        if k not in st.session_state: st.session_state[k] = v
ensure_state()

# --- GitHub API í•¨ìˆ˜ ---
def _gh_headers(token: str):
    return {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3+json"}

def github_upload_file(repo, branch, path_in_repo, local_path, token):
    url = f"https://api.github.com/repos/{repo}/contents/{path_in_repo}"
    with open(local_path, "rb") as f: content = base64.b64encode(f.read()).decode()
    headers = _gh_headers(token)
    get_resp = requests.get(url + f"?ref={branch}", headers=headers)
    sha = get_resp.json().get("sha") if get_resp.ok else None
    data = {"message": f"archive: {os.path.basename(path_in_repo)}", "content": content, "branch": branch}
    if sha: data["sha"] = sha
    resp = requests.put(url, headers=headers, json=data)
    resp.raise_for_status()
    return resp.json()

def github_list_dir(repo, branch, folder, token):
    url = f"https://api.github.com/repos/{repo}/contents/{folder}?ref={branch}"
    resp = requests.get(url, headers=_gh_headers(token))
    return [item['name'] for item in resp.json() if item['type'] == 'dir'] if resp.ok else []

def github_download_file(repo, branch, path_in_repo, token, local_path):
    url = f"https://api.github.com/repos/{repo}/contents/{path_in_repo}?ref={branch}"
    resp = requests.get(url, headers=_gh_headers(token))
    if resp.ok:
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        with open(local_path, "wb") as f: f.write(base64.b64decode(resp.json()["content"]))
        return True
    return False

def github_delete_folder(repo, branch, folder_path, token):
    contents_url = f"https://api.github.com/repos/{repo}/contents/{folder_path}?ref={branch}"
    headers = _gh_headers(token)
    resp = requests.get(contents_url, headers=headers)
    if not resp.ok: return
    for item in resp.json():
        delete_url = f"https://api.github.com/repos/{repo}/contents/{item['path']}"
        data = {"message": f"delete: {item['name']}", "sha": item['sha'], "branch": branch}
        requests.delete(delete_url, headers=headers, json=data).raise_for_status()

def github_rename_session(old_name, new_name, token):
    contents_url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/sessions/{old_name}?ref={GITHUB_BRANCH}"
    resp = requests.get(contents_url, headers=_gh_headers(token)); resp.raise_for_status()
    files_to_move = resp.json()
    for item in files_to_move:
        filename = item['name']
        local_path = os.path.join(SESS_DIR, filename)
        if not github_download_file(GITHUB_REPO, GITHUB_BRANCH, item['path'], token, local_path):
            raise Exception(f"Failed to download {filename} from {old_name}")
        github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{new_name}/{filename}", local_path, token)
    github_delete_folder(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{old_name}", token)

# --- ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜ ---
def _build_session_name() -> str:
    # ë®ì–´ì“°ê¸° ë¡œì§: ì´ë¯¸ ë¶ˆëŸ¬ì˜¨ ì„¸ì…˜ì´ ìˆë‹¤ë©´ ê·¸ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if st.session_state.get("loaded_session_name"):
        return st.session_state.loaded_session_name

    # 1. í˜„ì¬ ë¶„ì„ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜´
    schema = st.session_state.get("last_schema", {})
    kw = (schema.get("keywords", ["NoKeyword"]))[0]
    kw_slug = re.sub(r'[^\w-]', '', kw.replace(' ', '_'))[:20]

    # 2. GitHubì—ì„œ í˜„ì¬ í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì„¸ì…˜ ëª©ë¡ì„ ê°€ì ¸ì˜´
    if GITHUB_TOKEN and GITHUB_REPO:
        try:
            all_sessions = github_list_dir(GITHUB_REPO, GITHUB_BRANCH, "sessions", GITHUB_TOKEN)
            
            # 3. í˜„ì¬ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ì„¸ì…˜ë“¤ë§Œ í•„í„°ë§
            keyword_sessions = [s for s in all_sessions if s.startswith(f"{kw_slug}_")]

            # 4. í•„í„°ë§ëœ ì„¸ì…˜ ì´ë¦„ì—ì„œ ìˆ«ì ë¶€ë¶„ì„ ì¶”ì¶œí•˜ì—¬ ê°€ì¥ í° ìˆ«ìë¥¼ ì°¾ìŒ
            max_num = 0
            for sess_name in keyword_sessions:
                try:
                    num_part = sess_name.rsplit('_', 1)[-1]
                    if num_part.isdigit():
                        max_num = max(max_num, int(num_part))
                except (IndexError, ValueError):
                    continue # ìˆ«ì í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ë¬´ì‹œ

            # 5. ìƒˆ ì„¸ì…˜ ì´ë¦„ ìƒì„± (ê°€ì¥ í° ìˆ«ì + 1)
            new_num = max_num + 1
            return f"{kw_slug}_{new_num}"

        except Exception:
            # GitHub API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ê¸°ì¡´ ì‹œê°„ ê¸°ë°˜ ì´ë¦„ìœ¼ë¡œ ëŒ€ì²´
            return f"{kw_slug}_{now_kst().strftime('%Y-%m-%d_%H%M')}"
    else:
        # GitHub ì„¤ì •ì´ ì—†ì„ ê²½ìš°, ê¸°ì¡´ ì‹œê°„ ê¸°ë°˜ ì´ë¦„ ì‚¬ìš©
        return f"{kw_slug}_{now_kst().strftime('%Y-%m-%d_%H%M')}"

def save_current_session_to_github():
    if not all([GITHUB_REPO, GITHUB_TOKEN, st.session_state.chat, st.session_state.last_csv]):
        return False, "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ GitHub ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    sess_name = _build_session_name()
    local_dir = os.path.join(SESS_DIR, sess_name); os.makedirs(local_dir, exist_ok=True)
    try:
        meta_path = os.path.join(local_dir, "qa.json")
        meta_data = {"chat": st.session_state.chat, "last_schema": st.session_state.last_schema, "sample_text": st.session_state.sample_text}
        with open(meta_path, "w", encoding="utf-8") as f: json.dump(meta_data, f, ensure_ascii=False, indent=2)
        comments_path, videos_path = os.path.join(local_dir, "comments.csv"), os.path.join(local_dir, "videos.csv")
        os.system(f'cp "{st.session_state.last_csv}" "{comments_path}"')
        if st.session_state.last_df is not None: st.session_state.last_df.to_csv(videos_path, index=False, encoding="utf-8-sig")
        github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/qa.json", meta_path, GITHUB_TOKEN)
        github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/comments.csv", comments_path, GITHUB_TOKEN)
        if os.path.exists(videos_path): github_upload_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/videos.csv", videos_path, GITHUB_TOKEN)
        st.session_state.loaded_session_name = sess_name
        return True, sess_name
    except Exception as e: return False, f"ì €ì¥ ì‹¤íŒ¨: {e}"

def load_session_from_github(sess_name: str):
    with st.spinner(f"ì„¸ì…˜ '{sess_name}' ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        try:
            local_dir = os.path.join(SESS_DIR, sess_name)
            qa_ok = github_download_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/qa.json", GITHUB_TOKEN, os.path.join(local_dir, "qa.json"))
            comments_ok = github_download_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/comments.csv", GITHUB_TOKEN, os.path.join(local_dir, "comments.csv"))
            videos_ok = github_download_file(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}/videos.csv", GITHUB_TOKEN, os.path.join(local_dir, "videos.csv"))
            if not (qa_ok and comments_ok): st.error("ì„¸ì…˜ í•µì‹¬ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."); return
            st.session_state.clear(); ensure_state()
            with open(os.path.join(local_dir, "qa.json"), "r", encoding="utf-8") as f: meta = json.load(f)
            st.session_state.update({
                "chat": meta.get("chat", []), "last_schema": meta.get("last_schema", None),
                "last_csv": os.path.join(local_dir, "comments.csv"),
                "last_df": pd.read_csv(os.path.join(local_dir, "videos.csv")) if videos_ok and os.path.exists(os.path.join(local_dir, "videos.csv")) else pd.DataFrame(),
                "loaded_session_name": sess_name, "sample_text": meta.get("sample_text", "")
            })
        except Exception as e: st.error(f"ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")

if 'session_to_load' in st.session_state: load_session_from_github(st.session_state.pop('session_to_load')); st.rerun()
if 'session_to_delete' in st.session_state:
    sess_name = st.session_state.pop('session_to_delete')
    with st.spinner(f"ì„¸ì…˜ '{sess_name}' ì‚­ì œ ì¤‘..."): github_delete_folder(GITHUB_REPO, GITHUB_BRANCH, f"sessions/{sess_name}", GITHUB_TOKEN)
    st.success(f"ì„¸ì…˜ ì‚­ì œ ì™„ë£Œ."); time.sleep(1); st.rerun()
if 'session_to_rename' in st.session_state:
    old, new = st.session_state.pop('session_to_rename')
    if old and new and old != new:
        with st.spinner(f"ì´ë¦„ ë³€ê²½ ì¤‘..."):
            try: github_rename_session(old, new, GITHUB_TOKEN); st.success("ì´ë¦„ ë³€ê²½ ì™„ë£Œ!")
            except Exception as e: st.error(f"ë³€ê²½ ì‹¤íŒ¨: {e}")
        time.sleep(1); st.rerun()

# -------------------- ì‚¬ì´ë“œë°” --------------------
with st.sidebar:
    st.markdown(f'<h2 style="font-weight: 600; font-size: 1.6rem; margin-bottom: 1.5rem; background: -webkit-linear-gradient(45deg, #4285F4, #9B72CB, #D96570, #F2A60C); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„: AI ì±—ë´‡</h2>', unsafe_allow_html=True)
    st.caption("ë¬¸ì˜: ë¯¸ë””ì–´)ë””ì§€í„¸ë§ˆì¼€íŒ… ë°ì´í„°íŒŒíŠ¸")
    st.markdown("""<style>[data-testid="stSidebarUserContent"] { display: flex; flex-direction: column; height: calc(100vh - 4rem); } .sidebar-top-section { flex-grow: 1; overflow-y: auto; } .sidebar-bottom-section { flex-shrink: 0; }</style>""", unsafe_allow_html=True)
    st.markdown('<div class="sidebar-top-section">', unsafe_allow_html=True)
    
    st.markdown('<div class="new-chat-btn">', unsafe_allow_html=True)
    if st.button("âœ¨ ìƒˆ ì±„íŒ…", use_container_width=True): st.session_state.clear(); st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)

    if st.session_state.chat and st.session_state.last_csv:
        if st.button("ğŸ’¾ í˜„ì¬ ëŒ€í™” ì €ì¥", use_container_width=True):
            with st.spinner("ì„¸ì…˜ ì €ì¥ ì¤‘..."): success, result = save_current_session_to_github()
            if success: st.success(f"'{result}' ì €ì¥ ì™„ë£Œ!"); time.sleep(2); st.rerun()
            else: st.error(result)
    st.markdown("---"); st.markdown("#### ëŒ€í™” ê¸°ë¡")
    if not all([GITHUB_TOKEN, GITHUB_REPO]): st.caption("GitHub ì„¤ì •ì´ Secretsì— ì—†ìŠµë‹ˆë‹¤.")
    else:
        try:
            sessions = sorted(github_list_dir(GITHUB_REPO, GITHUB_BRANCH, "sessions", GITHUB_TOKEN), reverse=True)
            if not sessions: st.caption("ì €ì¥ëœ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                editing_session = st.session_state.get("editing_session", None)
                st.markdown('<div class="session-list">', unsafe_allow_html=True)
                for sess in sessions:
                    if sess == editing_session:
                        new_name = st.text_input("ìƒˆ ì´ë¦„:", value=sess, key=f"new_name_{sess}")
                        c1, c2 = st.columns(2)
                        if c1.button("âœ…", key=f"save_{sess}"): st.session_state.session_to_rename = (sess, new_name); st.session_state.pop('editing_session', None); st.rerun()
                        if c2.button("âŒ", key=f"cancel_{sess}"): st.session_state.pop('editing_session', None); st.rerun()
                    else:
                        c1, c2, c3 = st.columns([0.7, 0.15, 0.15])
                        if c1.button(sess, key=f"sess_{sess}", use_container_width=True): st.session_state.session_to_load = sess; st.rerun()
                        if c2.button("âœï¸", key=f"edit_{sess}"): st.session_state.editing_session = sess; st.rerun()
                        if c3.button("ğŸ—‘ï¸", key=f"del_{sess}"): st.session_state.session_to_delete = sess; st.rerun()
                st.markdown('</div>', unsafe_allow_html=True)
        except Exception as e: st.error("ê¸°ë¡ ë¡œë”© ì‹¤íŒ¨")
    st.markdown('</div>', unsafe_allow_html=True)


# -------------------- ë¡œì§ (ì´í•˜ ë³µì›) --------------------
def scroll_to_bottom(): st_html("<script> let last_message = document.querySelectorAll('.stChatMessage'); if (last_message.length > 0) { last_message[last_message.length - 1].scrollIntoView({behavior: 'smooth'}); } </script>", height=0)
def render_metadata_and_downloads():
    if not (schema := st.session_state.get("last_schema")): return
    kw_main, (start_iso, end_iso) = schema.get("keywords", []), (schema.get('start_iso', ''), schema.get('end_iso', ''))
    try: start_dt_str, end_dt_str = datetime.fromisoformat(start_iso).astimezone(KST).strftime('%Y-%m-%d %H:%M'), datetime.fromisoformat(end_iso).astimezone(KST).strftime('%Y-%m-%d %H:%M')
    except (ValueError, TypeError): start_dt_str, end_dt_str = (start_iso.split('T')[0] if start_iso else ""), (end_iso.split('T')[0] if end_iso else "")
    with st.container(border=True):
        st.markdown(f"""<div style="font-size:14px; color:#4b5563; line-height: 1.8;"><span style='font-weight:600;'>í‚¤ì›Œë“œ:</span> {', '.join(kw_main) if kw_main else '(ì—†ìŒ)'}<br><span style='font-weight:600;'>ê¸°ê°„:</span> {start_dt_str} ~ {end_dt_str} (KST)</div>""", unsafe_allow_html=True)
        csv_path, df_videos = st.session_state.get("last_csv"), st.session_state.get("last_df")
        if csv_path and os.path.exists(csv_path) and df_videos is not None and not df_videos.empty:
            with open(csv_path, "rb") as f: comment_csv_data = f.read()
            buffer = io.BytesIO(); df_videos.to_csv(buffer, index=False, encoding="utf-8-sig"); video_csv_data = buffer.getvalue()
            keywords_str = "_".join(kw_main).replace(" ", "_") if kw_main else "data"; now_str = now_kst().strftime('%Y%m%d')
            col1, col2, col3, _ = st.columns([1.1, 1.2, 1.2, 6.5])
            col1.markdown("<div style='font-size:14px; color:#4b5563; font-weight:600; padding-top: 5px;'>ë‹¤ìš´ë¡œë“œ:</div>", unsafe_allow_html=True)
            with col2: st.download_button("ì „ì²´ëŒ“ê¸€", comment_csv_data, f"comments_{keywords_str}_{now_str}.csv", "text/csv")
            with col3: st.download_button("ì˜ìƒëª©ë¡", video_csv_data, f"videos_{keywords_str}_{now_str}.csv", "text/csv")
def render_chat():
    for msg in st.session_state.chat:
        with st.chat_message(msg["role"]): st.markdown(msg["content"])

class RotatingKeys:
    def __init__(self, keys, state_key: str, on_rotate=None):
        self.keys, self.state_key, self.on_rotate = [k.strip() for k in (keys or []) if isinstance(k, str) and k.strip()][:10], state_key, on_rotate
        idx = st.session_state.get(state_key, 0)
        self.idx = 0 if not self.keys else (idx % len(self.keys))
        st.session_state[state_key] = self.idx
    def current(self): return self.keys[self.idx % len(self.keys)] if self.keys else None
    def rotate(self):
        if not self.keys: return
        self.idx = (self.idx + 1) % len(self.keys)
        st.session_state[self.state_key] = self.idx
        if callable(self.on_rotate): self.on_rotate(self.idx, self.current())

class RotatingYouTube:
    def __init__(self, keys, state_key="yt_key_idx"):
        self.rot, self.service = RotatingKeys(keys, state_key), None
        self._build()
    def _build(self):
        if not (key := self.rot.current()): raise RuntimeError("YouTube API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        self.service = build("youtube", "v3", developerKey=key)
    def execute(self, factory):
        try: return factory(self.service).execute()
        except HttpError as e:
            status, msg = getattr(getattr(e, 'resp', None), 'status', None), (getattr(e, 'content', b'').decode('utf-8', 'ignore') or '').lower()
            if status in (403, 429) and any(t in msg for t in ["quota", "rate", "limit"]) and len(YT_API_KEYS) > 1:
                self.rot.rotate(); self._build()
                return factory(self.service).execute()
            raise

LIGHT_PROMPT = (f"ì—­í• : ìœ íŠœë¸Œ ëŒ“ê¸€ ë°˜ì‘ ë¶„ì„ê¸°ì˜ ìì—°ì–´ í•´ì„ê°€.\nëª©í‘œ: í•œêµ­ì–´ ì…ë ¥ì—ì„œ [ê¸°ê°„(KST)]ê³¼ [í‚¤ì›Œë“œ/ì—”í‹°í‹°/ì˜µì…˜]ì„ í•´ì„.\nê·œì¹™:\n- ê¸°ê°„ì€ Asia/Seoul ê¸°ì¤€, ìƒëŒ€ê¸°ê°„ì˜ ì¢…ë£ŒëŠ” ì§€ê¸ˆ.\n- 'í‚¤ì›Œë“œ'ëŠ” ê²€ìƒ‰ì— ì‚¬ìš©í•  ê°€ì¥ í•µì‹¬ì ì¸ ì£¼ì œ(í”„ë¡œê·¸ë¨, ë¸Œëœë“œ ë“±) 1ê°œë¡œ í•œì •í•œë‹¤.\n- 'ì—”í‹°í‹°/ë³´ì¡°'ëŠ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ë‚´ì—ì„œ ë¶„ì„ì˜ ì´ˆì ì´ ë  ì¸ë¬¼, ì„¸ë¶€ ì£¼ì œ ë“±ì„ í¬í•¨í•œë‹¤.\n- ì˜µì…˜ íƒì§€: include_replies, channel_filter(any|official|unofficial), lang(ko|en|auto).\n\nì¶œë ¥(6ì¤„ ê³ ì •):\n- í•œ ì¤„ ìš”ì•½: <ë¬¸ì¥>\n- ê¸°ê°„(KST): <YYYY-MM-DDTHH:MM:SS+09:00> ~ <YYYY-MM-DDTHH:MM:SS+09:00>\n- í‚¤ì›Œë“œ: [<í•µì‹¬ í‚¤ì›Œë“œ 1ê°œ>]\n- ì—”í‹°í‹°/ë³´ì¡°: [<ì¸ë¬¼>, <ì„¸ë¶€ ì£¼ì œ ë“±>]\n- ì˜µì…˜: {{ include_replies: true|false, channel_filter: \"any|official|unofficial\", lang: \"ko|en|auto\" }}\n- ì›ë¬¸: {{USER_QUERY}}\n\ní˜„ì¬ KST: {to_iso_kst(now_kst())}\nì…ë ¥:\n{{USER_QUERY}}")
def call_gemini_rotating(model_name, keys, system_instruction, user_payload, timeout_s=120, max_tokens=2048) -> str:
    rk = RotatingKeys(keys, "gem_key_idx")
    if not rk.current(): raise RuntimeError("Gemini API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    for _ in range(len(rk.keys) or 1):
        try:
            genai.configure(api_key=rk.current())
            model = genai.GenerativeModel(model_name, generation_config={"temperature": 0.2, "max_output_tokens": max_tokens})
            resp = model.generate_content([system_instruction, user_payload], request_options={"timeout": timeout_s})
            if out := getattr(resp, "text", None): return out
            if c0 := (getattr(resp, "candidates", None) or [None])[0]:
                if p0 := (getattr(c0, "content", None) and getattr(c0.content, "parts", None) or [None])[0]:
                    if hasattr(p0, "text"): return p0.text
            return ""
        except Exception as e:
            if "429" in str(e).lower() and len(rk.keys) > 1: rk.rotate(); continue
            raise
    return ""
def parse_light_block_to_schema(light_text: str) -> dict:
    raw = (light_text or "").strip()
    m_time = re.search(r"ê¸°ê°„\(KST\)\s*:\s*([^~]+)~\s*([^\n]+)", raw)
    start_iso, end_iso = (m_time.group(1).strip(), m_time.group(2).strip()) if m_time else (None, None)
    m_kw = re.search(r"í‚¤ì›Œë“œ\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    keywords = [p.strip() for p in re.split(r"\s*,\s*", m_kw.group(1)) if p.strip()] if m_kw else []
    m_ent = re.search(r"ì—”í‹°í‹°/ë³´ì¡°\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    entities = [p.strip() for p in re.split(r"\s*,\s*", m_ent.group(1)) if p.strip()] if m_ent else []
    m_opt = re.search(r"ì˜µì…˜\s*:\s*\{(.*?)\}", raw, flags=re.DOTALL)
    options = {"include_replies": False, "channel_filter": "any", "lang": "auto"}
    if m_opt:
        blob = m_opt.group(1)
        if ir := re.search(r"include_replies\s*:\s*(true|false)", blob, re.I): options["include_replies"] = (ir.group(1).lower() == "true")
        if cf := re.search(r"channel_filter\s*:\s*\"(any|official|unofficial)\"", blob, re.I): options["channel_filter"] = cf.group(1)
        if lg := re.search(r"lang\s*:\s*\"(ko|en|auto)\"", blob, re.I): options["lang"] = lg.group(1)
    if not (start_iso and end_iso):
        end_dt, start_dt = now_kst(), now_kst() - timedelta(hours=24)
        start_iso, end_iso = to_iso_kst(start_dt), to_iso_kst(end_dt)
    if not keywords: keywords = [m[0]] if (m := re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", raw)) else ["ìœ íŠœë¸Œ"]
    return {"start_iso": start_iso, "end_iso": end_iso, "keywords": keywords, "entities": entities, "options": options, "raw": raw}
def yt_search_videos(rt, keyword, max_results, order="relevance", published_after=None, published_before=None):
    video_ids, token = [], None
    while len(video_ids) < max_results:
        params = {"q": keyword, "part": "id", "type": "video", "order": order, "maxResults": min(50, max_results - len(video_ids))}
        if published_after: params["publishedAfter"] = published_after
        if published_before: params["publishedBefore"] = published_before
        if token: params["pageToken"] = token
        resp = rt.execute(lambda s: s.search().list(**params))
        video_ids.extend(it["id"]["videoId"] for it in resp.get("items", []) if it["id"]["videoId"] not in video_ids)
        if not (token := resp.get("nextPageToken")): break
        time.sleep(0.25)
    return video_ids
def yt_video_statistics(rt, video_ids):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        if not batch: continue
        resp = rt.execute(lambda s: s.videos().list(part="statistics,snippet,contentDetails", id=",".join(batch)))
        for item in resp.get("items", []):
            stats, snip, cont = item.get("statistics", {}), item.get("snippet", {}), item.get("contentDetails", {})
            dur = cont.get("duration", "")
            h, m, s = re.search(r"(\d+)H", dur), re.search(r"(\d+)M", dur), re.search(r"(\d+)S", dur)
            dur_sec = (int(h.group(1)) * 3600 if h else 0) + (int(m.group(1)) * 60 if m else 0) + (int(s.group(1)) if s else 0)
            vid_id = item.get("id")
            rows.append({"video_id": vid_id, "video_url": f"https://www.youtube.com/watch?v={vid_id}", "title": snip.get("title", ""), "channelTitle": snip.get("channelTitle", ""), "publishedAt": snip.get("publishedAt", ""), "duration": dur, "shortType": "Shorts" if dur_sec <= 60 else "Clip", "viewCount": int(stats.get("viewCount", 0) or 0), "likeCount": int(stats.get("likeCount", 0) or 0), "commentCount": int(stats.get("commentCount", 0) or 0)})
        time.sleep(0.25)
    return rows
def yt_all_replies(rt, parent_id, video_id, title="", short_type="Clip", cap=None):
    replies, token = [], None
    while not (cap is not None and len(replies) >= cap):
        try: resp = rt.execute(lambda s: s.comments().list(part="snippet", parentId=parent_id, maxResults=100, pageToken=token, textFormat="plainText"))
        except HttpError: break
        for c in resp.get("items", []):
            sn = c["snippet"]
            replies.append({"video_id": video_id, "video_title": title, "shortType": short_type, "comment_id": c.get("id", ""), "parent_id": parent_id, "isReply": 1, "author": sn.get("authorDisplayName", ""), "text": sn.get("textDisplay", "") or "", "publishedAt": sn.get("publishedAt", ""), "likeCount": int(sn.get("likeCount", 0) or 0)})
        if not (token := resp.get("nextPageToken")): break
        time.sleep(0.2)
    return replies[:cap] if cap is not None else replies
def yt_all_comments_sync(rt, video_id, title="", short_type="Clip", include_replies=True, max_per_video=None):
    rows, token = [], None
    while not (max_per_video is not None and len(rows) >= max_per_video):
        try: resp = rt.execute(lambda s: s.commentThreads().list(part="snippet,replies", videoId=video_id, maxResults=100, pageToken=token, textFormat="plainText"))
        except HttpError: break
        for it in resp.get("items", []):
            top, thread_id = it["snippet"]["topLevelComment"]["snippet"], it["snippet"]["topLevelComment"]["id"]
            rows.append({"video_id": video_id, "video_title": title, "shortType": short_type, "comment_id": thread_id, "parent_id": "", "isReply": 0, "author": top.get("authorDisplayName", ""), "text": top.get("textDisplay", "") or "", "publishedAt": top.get("publishedAt", ""), "likeCount": int(top.get("likeCount", 0) or 0)})
            if include_replies and int(it["snippet"].get("totalReplyCount", 0) or 0) > 0:
                cap = None if max_per_video is None else max(0, max_per_video - len(rows))
                if cap == 0: break
                rows.extend(yt_all_replies(rt, thread_id, video_id, title, short_type, cap=cap))
        if not (token := resp.get("nextPageToken")): break
        time.sleep(0.2)
    return rows[:max_per_video] if max_per_video is not None else rows
def parallel_collect_comments_streaming(video_list, rt_keys, include_replies, max_total_comments, max_per_video, prog_bar):
    out_csv = os.path.join(BASE_DIR, f"collect_{uuid4().hex}.csv")
    wrote_header, total_written, done, total_videos = False, 0, 0, len(video_list)
    with ThreadPoolExecutor(max_workers=3) as ex:
        futures = {ex.submit(yt_all_comments_sync, RotatingYouTube(rt_keys), v["video_id"], v.get("title", ""), v.get("shortType", "Clip"), include_replies, max_per_video): v for v in video_list}
        for f in as_completed(futures):
            try:
                if comm := f.result():
                    dfc = pd.DataFrame(comm)
                    dfc.to_csv(out_csv, index=False, mode="a" if wrote_header else "w", header=not wrote_header, encoding="utf-8-sig")
                    wrote_header, total_written = True, total_written + len(dfc)
            except Exception: pass
            done += 1
            prog_bar.progress(min(0.90, 0.50 + (done / total_videos) * 0.40 if total_videos > 0 else 0.50), text="ëŒ“ê¸€ ìˆ˜ì§‘ì¤‘â€¦")
            if total_written >= max_total_comments: break
    return out_csv, total_written
def serialize_comments_for_llm_from_file(csv_path: str, max_chars_per_comment=280, max_total_chars=420_000):
    if not os.path.exists(csv_path): return "", 0, 0
    try: df_all = pd.read_csv(csv_path)
    except Exception: return "", 0, 0
    if df_all.empty: return "", 0, 0
    df_top_likes = df_all.sort_values("likeCount", ascending=False).head(1000)
    df_remaining = df_all.drop(df_top_likes.index)
    df_random = df_remaining.sample(n=min(1000, len(df_remaining))) if not df_remaining.empty else pd.DataFrame()
    df_sample, lines, total_chars = pd.concat([df_top_likes, df_random]), [], 0
    for _, r in df_sample.iterrows():
        if total_chars >= max_total_chars: break
        text = str(r.get("text", "") or "").replace("\n", " ")
        line = f"[{'R' if int(r.get('isReply', 0)) == 1 else 'T'}|â™¥{int(r.get('likeCount', 0))}] {str(r.get('author', '')).replace('\n', ' ')}: {text[:max_chars_per_comment] + 'â€¦' if len(text) > max_chars_per_comment else text}"
        if total_chars + len(line) + 1 > max_total_chars: break
        lines.append(line); total_chars += len(line) + 1
    return "\n".join(lines), len(lines), total_chars
TITLE_LINE_RE, HEADER_DUP_RE = re.compile(r"^\s{0,3}#{1,6}\s+.*$"), re.compile(r"ìœ íŠœë¸Œ\s*ëŒ“ê¸€\s*ë¶„ì„.*", re.IGNORECASE)
def tidy_answer(md: str) -> str:
    if not md: return md
    lines = [line for line in md.splitlines() if not (TITLE_LINE_RE.match(line) or HEADER_DUP_RE.search(line))]
    cleaned, prev_blank = [], False
    for l in lines:
        is_blank = not l.strip()
        if is_blank and prev_blank: continue
        cleaned.append(l); prev_blank = is_blank
    return "\n".join(cleaned).strip()
def run_pipeline_first_turn(user_query: str):
    prog_bar = st.progress(0, text="ì¤€ë¹„ ì¤‘â€¦")
    if not GEMINI_API_KEYS: return "ì˜¤ë¥˜: Gemini API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    prog_bar.progress(0.05, text="í•´ì„ì¤‘â€¦")
    light = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, "", LIGHT_PROMPT.replace("{USER_QUERY}", user_query))
    schema = parse_light_block_to_schema(light)
    st.session_state["last_schema"] = schema
    prog_bar.progress(0.10, text="ì˜ìƒ ìˆ˜ì§‘ì¤‘â€¦")
    if not YT_API_KEYS: return "ì˜¤ë¥˜: YouTube API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    rt = RotatingYouTube(YT_API_KEYS)
    start_dt, end_dt = datetime.fromisoformat(schema["start_iso"]), datetime.fromisoformat(schema["end_iso"])
    kw_main, kw_ent = schema.get("keywords", []), schema.get("entities", [])
    all_ids = []
    for base_kw in (kw_main or ["ìœ íŠœë¸Œ"]):
        all_ids.extend(yt_search_videos(rt, base_kw, 60, "relevance", kst_to_rfc3339_utc(start_dt), kst_to_rfc3339_utc(end_dt)))
        for e in kw_ent: all_ids.extend(yt_search_videos(rt, f"{base_kw} {e}", 30, "relevance", kst_to_rfc3339_utc(start_dt), kst_to_rfc3339_utc(end_dt)))
    all_ids = list(dict.fromkeys(all_ids))
    prog_bar.progress(0.40, text="ëŒ“ê¸€ ìˆ˜ì§‘ ì¤€ë¹„ì¤‘â€¦")
    df_stats = pd.DataFrame(yt_video_statistics(rt, all_ids))
    st.session_state["last_df"] = df_stats
    csv_path, total_cnt = parallel_collect_comments_streaming(df_stats.to_dict('records'), YT_API_KEYS, bool(schema.get("options", {}).get("include_replies")), MAX_TOTAL_COMMENTS, MAX_COMMENTS_PER_VID, prog_bar)
    st.session_state["last_csv"] = csv_path
    if total_cnt == 0: return "ì§€ì • ê¸°ê°„/í‚¤ì›Œë“œì—ì„œ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ ì‹œë„í•´ ë³´ì„¸ìš”."
    prog_bar.progress(0.90, text="AI ë¶„ì„ì¤‘â€¦")
    sample_text, _, _ = serialize_comments_for_llm_from_file(csv_path)
    st.session_state["sample_text"] = sample_text
    sys = ("ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë¨¼ì € [ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸]ì„ í™•ì¸í•˜ì—¬ ë¶„ì„ì˜ í•µì‹¬ ê´€ì (ì˜ˆ: íŠ¹ì • ì¸ë¬¼ ì¤‘ì‹¬, ê¸/ë¶€ì • ë°˜ì‘ ë“±)ì„ íŒŒì•…í•˜ë¼. ê·¸ ë‹¤ìŒ, ì£¼ì–´ì§„ ëŒ“ê¸€ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê´€ì ì— ë§ì¶° í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í•­ëª©í™”í•˜ê³ , ê¸/ë¶€/ì¤‘ ë¹„ìœ¨ê³¼ ëŒ€í‘œ ì½”ë©˜íŠ¸(10ê°œ ë¯¸ë§Œ)ë¥¼ ì œì‹œí•˜ë¼. ë‹¨, ì ˆëŒ€ë¡œ ë™ì¼í•œ ë‚´ìš©ì´ë‚˜ ë¬¸êµ¬ë¥¼ ë°˜ë³µí•´ì„œ ì¶œë ¥í•´ì„œëŠ” ì•ˆ ëœë‹¤.")
    payload = (f"[ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸]: {user_query}\n\n[í‚¤ì›Œë“œ]: {', '.join(kw_main)}\n[ì—”í‹°í‹°]: {', '.join(kw_ent)}\n[ê¸°ê°„(KST)]: {schema['start_iso']} ~ {schema['end_iso']}\n\n[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{sample_text}\n")
    answer_md_raw = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, sys, payload)
    prog_bar.progress(1.0, text="ì™„ë£Œ"); time.sleep(0.5); prog_bar.empty()
    return tidy_answer(answer_md_raw)


def run_followup_turn(user_query: str):
    if not (schema := st.session_state.get("last_schema")): return "ì˜¤ë¥˜: ì´ì „ ë¶„ì„ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ì±„íŒ…ì„ ì‹œì‘í•´ì£¼ì„¸ìš”."
    
    sample_text = st.session_state.get("sample_text", "")
    context = "\n".join(f"[ì´ì „ {'Q' if m['role'] == 'user' else 'A'}]: {m['content']}" for m in st.session_state["chat"][-10:])
    
    # [ìˆ˜ì •] ì±—ë´‡ì˜ ëŒ€í™” íë¦„ì„ ê°•ì¡°í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    sys = (
        "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ í•µì‹¬ë§Œ ë‹µë³€í•˜ëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ ë¶„ì„ ì±—ë´‡ì´ë‹¤.\n"
        "--- ì¤‘ìš” ê·œì¹™ ---\n"
        "1. **ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì´ ìµœìš°ì„ **: ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ **'ë¬´ì—‡(What)/ì–´ë–»ê²Œ(How)'**ì— ëŒ€í•œ ë‚´ìš©(ì •ì„±)ì„ ë¬»ëŠ”ì§€, **'ëª‡ ê°œ/ë¹„ìœ¨(How many)'**ì— ëŒ€í•œ ìˆ˜ì¹˜(ì •ëŸ‰)ë¥¼ ë¬»ëŠ”ì§€ ë¨¼ì € ëª…í™•íˆ êµ¬ë¶„í•˜ë¼.\n\n"
        "2. **'ë‚´ìš©(ì •ì„±)'ì„ ë¬¼ì—ˆì„ ë•Œ**: 'ì—°ê¸°ë ¥ ì–´ë•Œ?', 'ì–´ë–¤ ë‚´ìš©ì´ì•¼?' ì™€ ê°™ì€ ì§ˆë¬¸ì—ëŠ” **ì ˆëŒ€ ìˆ«ìë¡œë§Œ ë‹µí•˜ì§€ ë§ˆë¼.** ë°˜ë“œì‹œ `[ëŒ“ê¸€ ìƒ˜í”Œ]`ì—ì„œ ê´€ë ¨ëœ ì‹¤ì œ ëŒ“ê¸€ ë‚´ìš©ì„ ì°¾ì•„ **í•µì‹¬ ë°˜ì‘ì„ ìš”ì•½**í•˜ê³ , **ì£¼ìš” ëŒ“ê¸€ì„ 1~3ê°œ ì¸ìš©í•˜ì—¬ ê·¼ê±°ë¡œ ì œì‹œ**í•´ì•¼ í•œë‹¤.\n\n"
        "3. **'ìˆ˜ì¹˜(ì •ëŸ‰)'ë¥¼ ë¬¼ì—ˆì„ ë•Œ**: 'ëª‡ ê°œì•¼?', 'ë¹„ì¤‘ì´ ì–´ë•Œ?' ì™€ ê°™ì€ ì§ˆë¬¸ì—ëŠ” `[ëŒ“ê¸€ ìƒ˜í”Œ]` ë‚´ì—ì„œ í‚¤ì›Œë“œ ì–¸ê¸‰ íšŸìˆ˜ë¥¼ ì§ì ‘ ì„¸ì–´ **'ì•½ OíšŒ ì–¸ê¸‰ë©ë‹ˆë‹¤' ë˜ëŠ” 'Aê°€ Bë³´ë‹¤ ë” ë§ì´ ì–¸ê¸‰ë©ë‹ˆë‹¤'** ì™€ ê°™ì´ ìˆ«ìë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ë¼.\n\n"
        "4. **ë™ë¬¸ì„œë‹µ ì ˆëŒ€ ê¸ˆì§€**: ë§Œì•½ ì§ì „ ë‹µë³€ì—ì„œ 'ì–¸ê¸‰ íšŸìˆ˜ëŠ” 10íšŒì…ë‹ˆë‹¤'ë¼ê³  ì´ë¯¸ ë‹µí–ˆë‹¤ë©´, ì‚¬ìš©ìê°€ ë‹¤ì‹œ 'ë‚´ìš©ì´ ì–´ë•Œ?'ë¼ê³  ë¬¼ì—ˆì„ ë•Œ **ì ˆëŒ€ '10íšŒ ì–¸ê¸‰ë©ë‹ˆë‹¤'ë¼ê³  ë°˜ë³µí•˜ì§€ ë§ˆë¼.** ì‚¬ìš©ìëŠ” ì´ì œ ê·¸ 10ê°œì˜ 'ë‚´ìš©'ì„ ê¶ê¸ˆí•´í•˜ëŠ” ê²ƒì´ë‹¤.\n\n"
        "5. **ì–¸ê¸‰ ê¸ˆì§€**: ë‹µë³€ ì‹œ 'ëŒ“ê¸€ ìƒ˜í”Œ'ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆë¼."
        
    )

    payload = f"{context}\n\n[í˜„ì¬ ì§ˆë¬¸]: {user_query}\n[ê¸°ê°„(KST)]: {schema.get('start_iso', '?')} ~ {schema.get('end_iso', '?')}\n\n[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{sample_text}\n"
    
    with st.spinner("ğŸ’¬ AIê°€ ë‹µë³€ì„ êµ¬ì„± ì¤‘ì…ë‹ˆë‹¤..."):
        response = tidy_answer(call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, sys, payload))
        
    return response

# -------------------- ë©”ì¸ í™”ë©´ ë° ì‹¤í–‰ ë¡œì§ --------------------
if not st.session_state.chat:
    st.markdown("""<div style="display: flex; flex-direction: column; align-items: center; justify-content: center; text-align: center; height: 70vh;"><h1 style="font-size: 3.5rem; font-weight: 600; background: -webkit-linear-gradient(45deg, #4285F4, #9B72CB, #D96570, #F2A60C); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„: AI ì±—ë´‡</h1><p style="font-size: 1.2rem; color: #4b5563;">ë“œë¼ë§ˆ, ë°°ìš°ë¥¼ ì£¼ì œë¡œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”</p><div style="margin-top: 3rem; padding: 1rem 1.5rem; border: 1px solid #e5e7eb; border-radius: 12px; background-color: #fafafa; max-width: 600px;"><h4 style="margin-bottom: 1rem; font-weight: 600;">âš ï¸ ì‚¬ìš© ì£¼ì˜ì‚¬í•­</h4><ol style="text-align: left; padding-left: 20px;"><li><strong>ì²« ì§ˆë¬¸ ì‹œ</strong> ëŒ“ê¸€ ìˆ˜ì§‘ ë° AI ë¶„ì„ì— ë‹¤ì†Œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li><li>í•œ ì„¸ì…˜ì—ì„œëŠ” <strong>í•˜ë‚˜ì˜ ì£¼ì œ</strong>ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ë§Œ ì§„í–‰í•´ì•¼ ë¶„ì„ ì •í™•ë„ê°€ ìœ ì§€ë©ë‹ˆë‹¤.</li><li>ì²« ì§ˆë¬¸ì—ëŠ” ê¸°ê°„ì„ ëª…ì‹œí•´ì£¼ì„¸ìš” (ex.ìµœê·¼ 48ì‹œê°„ / 5ì›” 1ì¼ë¶€í„°).</li></ol></div></div>""", unsafe_allow_html=True)
else:
    render_metadata_and_downloads(); render_chat(); scroll_to_bottom()
if prompt := st.chat_input("ì˜ˆ) ìµœê·¼ 24ì‹œê°„ íƒœí’ìƒì‚¬ ê¹€ì¤€í˜¸ ë°˜ì‘ ìš”ì•½í•´ì¤˜"):
    st.session_state.chat.append({"role": "user", "content": prompt}); st.rerun()
if st.session_state.chat and st.session_state.chat[-1]["role"] == "user":
    user_query = st.session_state.chat[-1]["content"]
    response = run_pipeline_first_turn(user_query) if not st.session_state.get("last_csv") else run_followup_turn(user_query)
    st.session_state.chat.append({"role": "assistant", "content": response}); st.rerun()
