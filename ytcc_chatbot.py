# -*- coding: utf-8 -*-
# ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„ê¸° â€” ì±—ë´‡ ëª¨ë“œ (ë…ë¦½ ì•±)
# - ìì—°ì–´ í•œ ì¤„ â†’ (ê¸°ê°„/í‚¤ì›Œë“œ/ì˜µì…˜) í•´ì„ â†’ ì˜ìƒ ìˆ˜ì§‘ â†’ ëŒ“ê¸€ ìˆ˜ì§‘(ìŠ¤íŠ¸ë¦¬ë°) â†’ ìš”ì•½/ì‹œê°í™”
# - í•´ì„ì€ ììœ í˜•(ì œë¯¸ë‚˜ì´), ì–´ëŒ‘í„°ì—ì„œë§Œ ê·œê²©í™”(KST ISO, í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë“±)
# - Streamlit Cloud ê¸°ì¤€ /tmp ì‚¬ìš©, GitHub ì•„ì¹´ì´ë¸Œ ì˜µì…˜ ì œì™¸(ì‹¬í”Œ)

import streamlit as st
import pandas as pd
import os, re, io, gc, time, base64, json, requests
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from uuid import uuid4

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

import google.generativeai as genai

import plotly.express as px
from plotly import graph_objects as go
import circlify
import numpy as np

# =====================================================
# ê¸°ë³¸ ì„¤ì •
# =====================================================
st.set_page_config(page_title="ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„ê¸°: ì±—ë´‡ ëª¨ë“œ", layout="wide", initial_sidebar_state="collapsed")
st.title("ğŸ’¬ ìœ íŠœë¸Œ ëŒ“ê¸€ë¶„ì„ê¸°: ì±—ë´‡ ëª¨ë“œ (ë² íƒ€)")

# ===================== ê²½ë¡œ/ìƒìˆ˜ =====================
BASE_DIR = "/tmp"
os.makedirs(BASE_DIR, exist_ok=True)

MAX_TOTAL_COMMENTS = 120_000
MAX_COMMENTS_PER_VIDEO = 4_000
GEMINI_MODEL = st.secrets.get("GEMINI_MODEL", "gemini-2.0-flash-lite")
GEMINI_TIMEOUT = int(st.secrets.get("GEMINI_TIMEOUT", 120))
GEMINI_MAX_TOKENS = int(st.secrets.get("GEMINI_MAX_TOKENS", 2048))

# ===================== ë¹„ë°€í‚¤ =====================
_YT_FALLBACK = []
_GEM_FALLBACK = []
YT_API_KEYS = list(st.secrets.get("YT_API_KEYS", [])) or _YT_FALLBACK
GEMINI_API_KEYS = list(st.secrets.get("GEMINI_API_KEYS", [])) or _GEM_FALLBACK

# ===================== ìœ í‹¸/ê³µí†µ =====================
KST = timezone(timedelta(hours=9))

def now_kst() -> datetime:
    return datetime.now(tz=KST)

def to_iso_kst(dt: datetime) -> str:
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST)
    return dt.astimezone(KST).isoformat(timespec="seconds")

def kst_to_rfc3339_utc(dt_kst: datetime) -> str:
    if dt_kst.tzinfo is None:
        dt_kst = dt_kst.replace(tzinfo=KST)
    return dt_kst.astimezone(timezone.utc).isoformat().replace("+00:00","Z")

# ===================== Streamlit rerun í˜¸í™˜ =====================
def safe_rerun():
    fn = getattr(st, "rerun", None)
    if callable(fn):
        return fn()
    fn_old = getattr(st, "experimental_rerun", None)
    if callable(fn_old):
        return fn_old()
    raise RuntimeError("No rerun function available.")

# ===================== í‚¤ ë¡œí…Œì´í„° =====================
class RotatingKeys:
    def __init__(self, keys, state_key: str, on_rotate=None, treat_as_strings: bool = True):
        cleaned = []
        for k in (keys or []):
            if k is None: continue
            if treat_as_strings and isinstance(k, str):
                ks = k.strip()
                if ks: cleaned.append(ks)
            else:
                cleaned.append(k)
        self.keys = cleaned[:10]
        self.state_key = state_key
        self.on_rotate = on_rotate
        idx = st.session_state.get(state_key, 0)
        self.idx = 0 if not self.keys else (idx % len(self.keys))
        st.session_state[state_key] = self.idx
    def current(self):
        if not self.keys: return None
        return self.keys[self.idx % len(self.keys)]
    def rotate(self):
        if not self.keys: return
        self.idx = (self.idx + 1) % len(self.keys)
        st.session_state[self.state_key] = self.idx
        if callable(self.on_rotate): self.on_rotate(self.idx, self.current())

# ===================== YouTube ë˜í¼ =====================
class RotatingYouTube:
    def __init__(self, keys, state_key="yt_key_idx", log=None):
        self.rot = RotatingKeys(keys, state_key, on_rotate=lambda i, k: log and log(f"ğŸ” YouTube í‚¤ ì „í™˜ â†’ #{i+1}"))
        self.log = log
        self.service = None
        self._build_service()
    def _build_service(self):
        key = self.rot.current()
        if not key:
            raise RuntimeError("YouTube API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        self.service = build("youtube", "v3", developerKey=key)
    def _rotate_and_rebuild(self):
        self.rot.rotate(); self._build_service()
    def execute(self, request_factory, tries_per_key=2):
        attempts = 0
        max_attempts = len(self.rot.keys) if self.rot.keys else 1
        while attempts < max_attempts:
            try:
                req = request_factory(self.service)
                return req.execute()
            except HttpError as e:
                status = getattr(getattr(e, 'resp', None), 'status', None)
                msg = (getattr(e, 'content', b'').decode('utf-8', errors='ignore') or '').lower()
                quotaish = status in (403,429) and (('quota' in msg) or ('rate' in msg) or ('limit' in msg))
                if quotaish and len(self.rot.keys) > 1:
                    self._rotate_and_rebuild(); attempts += 1; continue
                raise

# ===================== Gemini í˜¸ì¶œ =====================
def is_gemini_quota_error(exc: Exception) -> bool:
    msg = (str(exc) or "").lower()
    return ("429" in msg) or ("too many requests" in msg) or ("rate limit" in msg) or ("resource exhausted" in msg) or ("quota" in msg)

def call_gemini_rotating(
    model_name: str,
    keys,
    system_instruction: str,
    user_payload: str,
    timeout_s: int = GEMINI_TIMEOUT,
    max_tokens: int = GEMINI_MAX_TOKENS,
    on_rotate=None
) -> str:
    rot = RotatingKeys(keys, state_key="gem_key_idx", on_rotate=lambda i, k: on_rotate and on_rotate(i, k))
    if not rot.current():
        raise RuntimeError("Gemini API Keyê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    attempts = 0
    max_attempts = len(rot.keys) if rot.keys else 1
    while attempts < max_attempts:
        try:
            genai.configure(api_key=rot.current())
            model = genai.GenerativeModel(
                model_name,
                generation_config={"temperature": 0.2, "max_output_tokens": max_tokens, "top_p": 0.9}
            )
            resp = model.generate_content([system_instruction, user_payload], request_options={"timeout": timeout_s})
            out = getattr(resp, "text", None)
            if not out and hasattr(resp, "candidates") and resp.candidates:
                c0 = resp.candidates[0]
                if hasattr(c0, "content") and getattr(c0.content, "parts", None):
                    p0 = c0.content.parts[0]
                    if hasattr(p0, "text"):
                        out = p0.text
            return out or ""
        except Exception as e:
            if is_gemini_quota_error(e) and len(rot.keys) > 1:
                rot.rotate(); attempts += 1; continue
            raise

# ===================== ìì—°ì–´ â†’ ë¼ì´íŠ¸ ìš”ì•½ ë¸”ë¡ í”„ë¡¬í”„íŠ¸ =====================
LIGHT_PROMPT = (
    "ì—­í• : ë‹¹ì‹ ì€ â€˜ìœ íŠœë¸Œ ëŒ“ê¸€ ë°˜ì‘ ë¶„ì„ê¸°â€™ë¥¼ ìœ„í•œ ìì—°ì–´ í•´ì„ê°€ë‹¤.\n"
    "ëª©í‘œ: ì‚¬ìš©ìê°€ í•œêµ­ì–´ë¡œ ë§í•œ ìš”ì²­ì—ì„œ [ê²€ìƒ‰ ê¸°ê°„]ê³¼ [ê²€ìƒ‰ í‚¤ì›Œë“œ(ì£¼ì œ/ì—”í‹°í‹°/ë³´ì¡°ì–´)]ë¥¼ ìµœëŒ€í•œ ì •í™•íˆ í•´ì„í•œë‹¤.\n"
    "ì›ì¹™:\n"
    "- ì‚¬ìš©ìì˜ í‘œí˜„ì„ ì¡´ì¤‘í•œë‹¤. ìì˜ì  ì¶•ì•½Â·ì‚­ì œ ê¸ˆì§€.\n"
    "- ê¸°ê°„ì€ í•œêµ­ í‘œì¤€ì‹œ(Asia/Seoul, +09:00) ê¸°ì¤€ìœ¼ë¡œ í•´ì„í•œë‹¤.\n"
    "- â€˜ìµœê·¼ Nì‹œê°„/ì¼/ì£¼/ê°œì›”/ë…„â€™ ê°™ì€ ìƒëŒ€ ê¸°ê°„ì€ ì¢…ë£Œì‹œì ì„ â€˜ì§€ê¸ˆâ€™ìœ¼ë¡œ ë³¸ë‹¤.\n"
    "- ì ˆëŒ€ ê¸°ê°„(ì˜ˆ: 2025-09-01~2025-09-07, ì–´ì œ 18ì‹œ~ì˜¤ëŠ˜ 9ì‹œ)ì€ ê·¸ëŒ€ë¡œ ê³„ì‚°í•œë‹¤.\n"
    "- ì‘í’ˆ/ë¸Œëœë“œ/ì‚¬ëŒ ì´ë¦„ì²˜ëŸ¼ ì˜ë¯¸ ìˆëŠ” ê³ ìœ ëª…ì‚¬ëŠ” ì›ë¬¸ í‘œê¸°ë¥¼ ë³´ì¡´í•œë‹¤.\n"
    "- ì˜µì…˜ì´ ìì—°ì–´ì— ìˆìœ¼ë©´ ê°ì§€í•œë‹¤: ëŒ€ëŒ“ê¸€ í¬í•¨/ì œì™¸, ê³µì‹ ì±„ë„ë§Œ/ë¹„ê³µì‹, ì–¸ì–´(í•œêµ­ì–´ë§Œ/ì˜ì–´ë§Œ/ìë™).\n\n"
    "ì¶œë ¥ í˜•ì‹(ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¼ì´íŠ¸ ìš”ì•½; ì´ ë¸”ë¡ë§Œ ê·œì¹™ì ìœ¼ë¡œ ì¨ë¼):\n"
    "- í•œ ì¤„ ìš”ì•½: <í•œ ë¬¸ì¥ìœ¼ë¡œ í•´ì„ ê²°ê³¼ ìš”ì•½>\n"
    "- ê¸°ê°„(KST): <YYYY-MM-DDTHH:MM:SS+09:00> ~ <YYYY-MM-DDTHH:MM:SS+09:00>\n"
    "- í‚¤ì›Œë“œ: [<ë©”ì¸ í‚¤ì›Œë“œ 1>, <ë©”ì¸ í‚¤ì›Œë“œ 2> ...]\n"
    "- ì—”í‹°í‹°/ë³´ì¡°: [<ì¸ë¬¼/ë³´ì¡° í‚¤ì›Œë“œë“¤, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´>]\n"
    "- ì˜µì…˜: { include_replies: true|false, channel_filter: \"any|official|unofficial\", lang: \"ko|en|auto\" }\n"
    "- ì›ë¬¸: {USER_QUERY}\n\n"
    f"ì§€ê¸ˆ ì‹œê°„ì€ KST ê¸°ì¤€ìœ¼ë¡œ \"{to_iso_kst(now_kst())}\" ì´ë‹¤.\n"
    "ì•„ë˜ ì‚¬ìš©ì ì…ë ¥ì„ í•´ì„í•˜ë¼:\n\n{USER_QUERY}"
)

# ===================== ë¼ì´íŠ¸ ë¸”ë¡ â†’ í‘œì¤€ ìŠ¤í‚¤ë§ˆ ì–´ëŒ‘í„° =====================
# í‘œì¤€ ìŠ¤í‚¤ë§ˆ: {start_iso, end_iso, keywords[], entities[], options{}, raw}

def parse_light_block_to_schema(light_text: str) -> dict:
    raw = (light_text or "").strip()
    # 1) ê° ë¼ì¸ ìº¡ì²˜
    # - ê¸°ê°„(KST): ... ~ ...
    m_time = re.search(r"ê¸°ê°„\(KST\)\s*:\s*([^~]+)~\s*([^\n]+)", raw)
    start_iso = end_iso = None
    if m_time:
        start_iso = m_time.group(1).strip()
        end_iso = m_time.group(2).strip()
    # - í‚¤ì›Œë“œ: [ ... ]
    m_kw = re.search(r"í‚¤ì›Œë“œ\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    keywords = []
    if m_kw:
        body = m_kw.group(1)
        # í•­ëª©ì€ ì‰¼í‘œë¡œ ë¶„ë¦¬, ê´„í˜¸ í›„ë³´ëŠ” ì œê±°í•˜ì—¬ ë©”ì¸í‘œê¸°ë¥¼ ìš°ì„  ë³´ì¡´
        for part in re.split(r"\s*,\s*", body):
            part = part.strip()
            if not part:
                continue
            # ê´„í˜¸ ë‚´ í›„ë³´(ì˜ˆ: íƒœí’ ìƒì‚¬(íƒœí’ìƒì‚¬)) â†’ ë°”ê¹¥í‘œê¸° ìš°ì„ 
            part = re.sub(r"\(.*?\)", "", part).strip()
            if part:
                keywords.append(part)
    # - ì—”í‹°í‹°/ë³´ì¡°: [ ... ]
    m_ent = re.search(r"ì—”í‹°í‹°/ë³´ì¡°\s*:\s*\[(.*?)\]", raw, flags=re.DOTALL)
    entities = []
    if m_ent:
        body = m_ent.group(1)
        for part in re.split(r"\s*,\s*", body):
            part = part.strip()
            if part:
                entities.append(part)
    # - ì˜µì…˜: { ... }
    m_opt = re.search(r"ì˜µì…˜\s*:\s*\{(.*?)\}", raw, flags=re.DOTALL)
    options = {"include_replies": False, "channel_filter": "any", "lang": "auto"}
    if m_opt:
        blob = m_opt.group(1)
        inc = re.search(r"include_replies\s*:\s*(true|false)", blob, re.IGNORECASE)
        if inc:
            options["include_replies"] = (inc.group(1).lower() == "true")
        ch = re.search(r"channel_filter\s*:\s*\"(any|official|unofficial)\"", blob, re.IGNORECASE)
        if ch:
            options["channel_filter"] = ch.group(1)
        lg = re.search(r"lang\s*:\s*\"(ko|en|auto)\"", blob, re.IGNORECASE)
        if lg:
            options["lang"] = lg.group(1)
    # ì•ˆì „ ë³´ì •
    if not start_iso or not end_iso:
        # ìƒëŒ€ê¸°ê°„ ëˆ„ë½ ë“± â†’ ê¸°ë³¸ ìµœê·¼ 24ì‹œê°„
        end_dt = now_kst(); start_dt = end_dt - timedelta(hours=24)
        start_iso, end_iso = to_iso_kst(start_dt), to_iso_kst(end_dt)
    # í‚¤ì›Œë“œ ë¹„ì—ˆì„ ë•Œ ì•ˆì „ê°’
    if not keywords:
        # ë”°ì˜´í‘œ ì•ˆ ìµœëŒ€ í† í° or ì „ì²´ ë¬¸ìì—´ì˜ ê¸´ í•œê¸€ í† í° ì‹œë„
        m = re.findall(r"[\"'â€œâ€â€˜â€™](.*?)[\"'â€œâ€â€˜â€™]", raw)
        if m:
            keywords = [s.strip() for s in m if s.strip()][:1]
        if not keywords:
            m2 = re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", raw)
            keywords = [m2[0]] if m2 else ["ìœ íŠœë¸Œ"]
    # ê³µë°±ì œê±°ëœ ë²„ì „ ë³´ì¡°(ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ), ë‹¤ë§Œ í‘œì¤€ ìŠ¤í‚¤ë§ˆì—” ì›ë¬¸í˜• ë³´ì¡´
    return {
        "start_iso": start_iso,
        "end_iso": end_iso,
        "keywords": keywords,
        "entities": entities,
        "options": options,
        "raw": raw,
    }

# ===================== YouTube ê²€ìƒ‰/í†µê³„ =====================
_YT_ID_RE = re.compile(r'^[A-Za-z0-9_-]{11}$')

def yt_search_videos(rt, keyword, max_results, order="relevance", published_after=None, published_before=None, log=None):
    video_ids, token = [], None
    while len(video_ids) < max_results:
        params = dict(q=keyword, part="id", type="video", order=order, maxResults=min(50, max_results - len(video_ids)))
        if published_after: params["publishedAfter"] = published_after
        if published_before: params["publishedBefore"] = published_before
        if token: params["pageToken"] = token
        resp = rt.execute(lambda s: s.search().list(**params))
        for it in resp.get("items", []):
            vid = it["id"]["videoId"]
            if vid not in video_ids: video_ids.append(vid)
        token = resp.get("nextPageToken")
        if not token: break
        if log: log(f"ê²€ìƒ‰ ì§„í–‰: {len(video_ids)}ê°œ")
        time.sleep(0.3)
    return video_ids

def yt_video_statistics(rt, video_ids, log=None):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i + 50]
        if not batch: continue
        resp = rt.execute(lambda s: s.videos().list(part="statistics,snippet,contentDetails", id=",".join(batch)))
        for item in resp.get("items", []):
            stats = item.get("statistics", {})
            snip = item.get("snippet", {})
            cont = item.get("contentDetails", {})
            dur_iso = cont.get("duration", "")
            def _dsec(dur: str):
                if not dur or not dur.startswith("P"): return None
                h = re.search(r"(\d+)H", dur); m = re.search(r"(\d+)M", dur); s = re.search(r"(\d+)S", dur)
                return (int(h.group(1)) if h else 0) * 3600 + (int(m.group(1)) if m else 0) * 60 + (int(s.group(1)) if s else 0)
            dur_sec = _dsec(dur_iso)
            short_type = "Shorts" if (dur_sec is not None and dur_sec <= 60) else "Clip"
            vid_id = item.get("id")
            rows.append({
                "video_id": vid_id,
                "video_url": f"https://www.youtube.com/watch?v={vid_id}",
                "title": snip.get("title", ""),
                "channelTitle": snip.get("channelTitle", ""),
                "publishedAt": snip.get("publishedAt", ""),
                "duration": dur_iso,
                "shortType": short_type,
                "viewCount": int(stats.get("viewCount", 0) or 0),
                "likeCount": int(stats.get("likeCount", 0) or 0),
                "commentCount": int(stats.get("commentCount", 0) or 0),
            })
        if log: log(f"í†µê³„ ë°°ì¹˜ {i // 50 + 1} ì™„ë£Œ")
        time.sleep(0.3)
    return rows

# ===================== ëŒ“ê¸€ ìˆ˜ì§‘(ìŠ¤ë ˆë“œ) + CSV ìŠ¤íŠ¸ë¦¬ë° =====================

def yt_all_replies(rt, parent_id, video_id, title="", short_type="Clip", log=None, cap=None):
    replies, token = [], None
    while True:
        if cap is not None and len(replies) >= cap:
            return replies[:cap]
        params = dict(part="snippet", parentId=parent_id, maxResults=100, pageToken=token, textFormat="plainText")
        try:
            resp = rt.execute(lambda s: s.comments().list(**params))
        except HttpError as e:
            if log: log(f"[ì˜¤ë¥˜] replies {video_id}/{parent_id}: {e}")
            break
        for c in resp.get("items", []):
            sn = c["snippet"]
            replies.append({
                "video_id": video_id, "video_title": title, "shortType": short_type,
                "comment_id": c.get("id", ""), "parent_id": parent_id, "isReply": 1,
                "author": sn.get("authorDisplayName", ""),
                "text": sn.get("textDisplay", "") or "",
                "publishedAt": sn.get("publishedAt", ""),
                "likeCount": int(sn.get("likeCount", 0) or 0),
            })
            if cap is not None and len(replies) >= cap:
                return replies[:cap]
        token = resp.get("nextPageToken")
        if not token: break
        time.sleep(0.2)
    return replies


def yt_all_comments_sync(rt, video_id, title="", short_type="Clip", include_replies=True, log=None, max_per_video: int | None = None):
    rows, token = [], None
    while True:
        if max_per_video is not None and len(rows) >= max_per_video:
            return rows[:max_per_video]
        params = dict(part="snippet,replies", videoId=video_id, maxResults=100, pageToken=token, textFormat="plainText")
        try:
            resp = rt.execute(lambda s: s.commentThreads().list(**params))
        except HttpError as e:
            if log: log(f"[ì˜¤ë¥˜] commentThreads {video_id}: {e}")
            break
        for it in resp.get("items", []):
            top = it["snippet"]["topLevelComment"]["snippet"]
            thread_id = it["snippet"]["topLevelComment"]["id"]
            total_replies = int(it["snippet"].get("totalReplyCount", 0) or 0)
            rows.append({
                "video_id": video_id, "video_title": title, "shortType": short_type,
                "comment_id": thread_id, "parent_id": "", "isReply": 0,
                "author": top.get("authorDisplayName", ""),
                "text": top.get("textDisplay", "") or "",
                "publishedAt": top.get("publishedAt", ""),
                "likeCount": int(top.get("likeCount", 0) or 0),
            })
            if include_replies and total_replies > 0:
                cap = None
                if max_per_video is not None:
                    cap = max(0, max_per_video - len(rows))
                if cap == 0:
                    return rows[:max_per_video]
                rows.extend(yt_all_replies(rt, thread_id, video_id, title, short_type, log, cap=cap))
                if max_per_video is not None and len(rows) >= max_per_video:
                    return rows[:max_per_video]
        token = resp.get("nextPageToken")
        if not token: break
        if log: log(f"  ëŒ“ê¸€ í˜ì´ì§€ ì§„í–‰, ëˆ„ê³„ {len(rows)}")
        time.sleep(0.2)
    return rows


def parallel_collect_comments_streaming(video_list, rt_keys, include_replies, max_total_comments, max_per_video, log_callback=None, prog_callback=None):
    out_csv = os.path.join(BASE_DIR, f"collect_{uuid4().hex}.csv")
    wrote_header = False
    total_written = 0
    total_videos = len(video_list)
    done_videos = 0
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(
                yt_all_comments_sync,
                RotatingYouTube(rt_keys),
                vid_info["video_id"],
                vid_info.get("title", ""),
                vid_info.get("shortType", "Clip"),
                include_replies,
                None,
                max_per_video
            ): vid_info for vid_info in video_list
        }
        for fut in as_completed(futures):
            vid_info = futures[fut]
            try:
                comments = fut.result()
                if comments:
                    df_chunk = pd.DataFrame(comments)
                    df_chunk.to_csv(
                        out_csv, index=False,
                        mode=("a" if wrote_header else "w"),
                        header=(not wrote_header),
                        encoding="utf-8-sig"
                    )
                    wrote_header = True
                    total_written += len(df_chunk)
                done_videos += 1
                if log_callback: log_callback(f"âœ… [{done_videos}/{total_videos}] {vid_info.get('title','')} - {len(comments):,}ê°œ ìˆ˜ì§‘")
                if prog_callback: prog_callback(done_videos / total_videos)
            except Exception as e:
                done_videos += 1
                if log_callback: log_callback(f"âŒ [{done_videos}/{total_videos}] {vid_info.get('title','')} - ì‹¤íŒ¨: {e}")
                if prog_callback: prog_callback(done_videos / total_videos)
            if total_written >= max_total_comments:
                if log_callback: log_callback(f"ìµœëŒ€ ìˆ˜ì§‘ í•œë„({max_total_comments:,}ê°œ) ë„ë‹¬, ì¤‘ë‹¨")
                break
    return out_csv, total_written

# ===================== LLMìš© ì§ë ¬í™”(ìƒ˜í”Œ) =====================

def serialize_comments_for_llm_from_file(csv_path: str, max_rows=1500, max_chars_per_comment=280, max_total_chars=420_000):
    if not csv_path or not os.path.exists(csv_path):
        return "", 0, 0
    lines, total = [], 0
    remaining = max_rows
    for chunk in pd.read_csv(csv_path, chunksize=120_000):
        if "likeCount" in chunk.columns:
            chunk = chunk.sort_values("likeCount", ascending=False)
        for _, r in chunk.iterrows():
            if remaining <= 0 or total >= max_total_chars:
                break
            is_reply = "R" if int(r.get("isReply", 0) or 0) == 1 else "T"
            author = str(r.get("author", "") or "").replace("\n", " ")
            likec = int(r.get("likeCount", 0) or 0)
            text = str(r.get("text", "") or "").replace("\n", " ")
            if len(text) > max_chars_per_comment:
                text = text[:max_chars_per_comment] + "â€¦"
            line = f"[{is_reply}|â™¥{likec}] {author}: {text}"
            if total + len(line) + 1 > max_total_chars:
                break
            lines.append(line)
            total += len(line) + 1
            remaining -= 1
        if remaining <= 0 or total >= max_total_chars:
            break
    return "\n".join(lines), len(lines), total

# ===================== ì •ëŸ‰ ì‹œê°í™”(ê°„ë‹¨íŒ) =====================

def timeseries_from_file(csv_path: str):
    if not csv_path or not os.path.exists(csv_path): return None, None
    tmin = None; tmax = None
    for chunk in pd.read_csv(csv_path, usecols=["publishedAt"], chunksize=200_000):
        dt = pd.to_datetime(chunk["publishedAt"], errors="coerce", utc=True)
        if dt.notna().any():
            lo, hi = dt.min(), dt.max()
            tmin = lo if (tmin is None or (lo < tmin)) else tmin
            tmax = hi if (tmax is None or (hi > tmax)) else tmax
    if tmin is None or tmax is None:
        return None, None
    span_hours = (tmax - tmin).total_seconds()/3600.0
    use_hour = (span_hours <= 48)

    agg = {}
    for chunk in pd.read_csv(csv_path, usecols=["publishedAt"], chunksize=200_000):
        dt = pd.to_datetime(chunk["publishedAt"], errors="coerce", utc=True).dt.tz_convert("Asia/Seoul")
        dt = dt.dropna()
        if dt.empty: continue
        bucket = (dt.dt.floor("H") if use_hour else dt.dt.floor("D"))
        vc = bucket.value_counts()
        for t, c in vc.items():
            agg[t] = agg.get(t, 0) + int(c)
    ts = pd.Series(agg).sort_index().rename("count").reset_index().rename(columns={"index":"bucket"})
    return ts, ("ì‹œê°„ë³„" if use_hour else "ì¼ìë³„")

# ===================== UI â€” ì…ë ¥/ì‹¤í–‰ =====================
with st.container(border=True):
    st.subheader("í•œ ì¤„ ìš”ì²­")
    user_query = st.text_input(
        "ì±—ë´‡ì—ê²Œ ë§í•˜ë“¯ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ) ìµœê·¼ 12ì‹œê°„ íƒœí’ìƒì‚¬ ê¹€ì¤€í˜¸ ëŒ“ê¸€ë°˜ì‘ ë¶„ì„í•´ì¤˜",
        key="cb_query",
    )
    colA, colB = st.columns([1,1])
    btn_parse = colA.button("ğŸ§­ í•´ì„ë§Œ", type="secondary")
    btn_run = colB.button("ğŸš€ ì¦‰ì‹œ ì‹¤í–‰", type="primary")

# ===================== í•´ì„ ë‹¨ê³„ =====================
light_block = None
schema = None
if btn_parse or btn_run:
    if not GEMINI_API_KEYS:
        st.error("Gemini API Keyê°€ ì—†ìŠµë‹ˆë‹¤. st.secretsì— GEMINI_API_KEYSë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    else:
        with st.status("ì œë¯¸ë‚˜ì´ í•´ì„ ì¤‘â€¦", expanded=True) as status:
            payload = LIGHT_PROMPT.replace("{USER_QUERY}", user_query or "").replace("{NOW_KST_ISO}", to_iso_kst(now_kst()))
            out = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, "", payload,
                                       timeout_s=GEMINI_TIMEOUT, max_tokens=GEMINI_MAX_TOKENS,
                                       on_rotate=lambda i, k: status.write(f"ğŸ” Gemini í‚¤ ì „í™˜ â†’ #{i+1}"))
            light_block = out
            st.markdown("#### ğŸ” ë¼ì´íŠ¸ ìš”ì•½ ë¸”ë¡ (Gemini ì›ë¬¸)")
            st.code(light_block or "(ë¹ˆ ì‘ë‹µ)")
            schema = parse_light_block_to_schema(light_block or "")
            st.markdown("#### ğŸ§± ê·œê²©í™” ìŠ¤í‚¤ë§ˆ")
            st.json(schema)
            status.update(label="í•´ì„ ì™„ë£Œ", state="complete")

# ===================== ì‹¤í–‰(ìˆ˜ì§‘â†’ìš”ì•½) =====================
if btn_run and schema:
    if not YT_API_KEYS:
        st.error("YouTube API Keyê°€ ì—†ìŠµë‹ˆë‹¤. st.secretsì— YT_API_KEYSë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    else:
        start_dt = datetime.fromisoformat(schema["start_iso"]).astimezone(KST)
        end_dt   = datetime.fromisoformat(schema["end_iso"]).astimezone(KST)
        kw_main  = schema.get("keywords", [])
        kw_entities = schema.get("entities", [])
        include_replies = bool(schema.get("options", {}).get("include_replies", False))

        # ê²€ìƒ‰ ê¸°ê°„ RFC3339(UTC)
        published_after = kst_to_rfc3339_utc(start_dt)
        published_before = kst_to_rfc3339_utc(end_dt)

        with st.status("ì˜ìƒ/ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘â€¦", expanded=True) as status:
            rt = RotatingYouTube(YT_API_KEYS, log=lambda m: status.write(m))
            all_ids = []
            # ë©”ì¸ í‚¤ì›Œë“œ + ì—”í‹°í‹° ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰ í­ì„ ë„“íŒ ë’¤ dedupe
            for base_kw in (kw_main or ["ìœ íŠœë¸Œ"]):
                ids = yt_search_videos(rt, base_kw, max_results=60, order="relevance",
                                       published_after=published_after, published_before=published_before,
                                       log=status.write)
                all_ids.extend(ids)
                # ì—”í‹°í‹° ê²°í•© ì¿¼ë¦¬ë„ ì‹œë„
                for e in (kw_entities or []):
                    q2 = f"{base_kw} {e}"
                    ids2 = yt_search_videos(rt, q2, max_results=30, order="relevance",
                                            published_after=published_after, published_before=published_before,
                                            log=None)
                    all_ids.extend(ids2)
            all_ids = list(dict.fromkeys(all_ids))
            status.write(f"ğŸï¸ ëŒ€ìƒ ì˜ìƒ: {len(all_ids)}ê°œ")

            stats = yt_video_statistics(rt, all_ids, log=status.write)
            df_stats = pd.DataFrame(stats)
            if not df_stats.empty and "publishedAt" in df_stats.columns:
                df_stats["publishedAt_kst"] = (
                    pd.to_datetime(df_stats["publishedAt"], errors="coerce", utc=True)
                    .dt.tz_convert("Asia/Seoul").dt.strftime("%Y-%m-%d %H:%M:%S")
                )
            st.dataframe(df_stats.head(20), use_container_width=True)

            status.write("ğŸ’¬ ëŒ“ê¸€ ìˆ˜ì§‘(ìŠ¤íŠ¸ë¦¬ë°)â€¦")
            video_list = df_stats.to_dict('records') if not df_stats.empty else []
            prog = st.progress(0, text="ìˆ˜ì§‘ ì§„í–‰ ì¤‘")
            log_ph = st.empty()
            csv_path, total_cnt = parallel_collect_comments_streaming(
                video_list=video_list,
                rt_keys=YT_API_KEYS,
                include_replies=include_replies,
                max_total_comments=MAX_TOTAL_COMMENTS,
                max_per_video=MAX_COMMENTS_PER_VIDEO,
                log_callback=log_ph.write,
                prog_callback=prog.progress
            )
            status.write(f"ì´ ëŒ“ê¸€ ìˆ˜ì§‘: {total_cnt:,}ê°œ")
            status.update(label="ìˆ˜ì§‘ ì™„ë£Œ", state="complete")

        if total_cnt == 0:
            st.warning("ìˆ˜ì§‘ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„/í‚¤ì›Œë“œë¥¼ ì¡°ì •í•´ ë³´ì„¸ìš”.")
        else:
            # ===== AI ìš”ì•½ =====
            st.markdown("---")
            st.subheader("ğŸ§  AI ìš”ì•½")
            a_text, _, _ = serialize_comments_for_llm_from_file(csv_path)
            system_instruction = (
                "ë„ˆëŠ” ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
                "ì•„ë˜ í‚¤ì›Œë“œ/ì—”í‹°í‹°ì™€ ì§€ì •ëœ ê¸°ê°„ ë‚´ ëŒ“ê¸€ ìƒ˜í”Œì„ ë°”íƒ•ìœ¼ë¡œ, ì „ë°˜ì  ë°˜ì‘ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼. "
                "í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í•­ëª©í™”í•˜ê³ , ê¸/ë¶€ì •/ì¤‘ë¦½ì˜ ëŒ€ëµì  ë¹„ìœ¨ê³¼ ëŒ€í‘œ ì½”ë©˜íŠ¸(10ê°œë¯¸ë§Œ)ë¥¼ ì˜ˆì‹œë¡œ ì œì‹œí•˜ë¼. "
                "ë°˜ë“œì‹œ ìƒ˜í”Œì„ ê·¼ê±°ë¡œ ì‘ì„±í•˜ë¼."
            )
            prompt_q = (
                f"[í‚¤ì›Œë“œ]: {', '.join(kw_main or [])}\n"
                f"[ì—”í‹°í‹°]: {', '.join(kw_entities or [])}\n"
                f"[ê¸°ê°„(KST)]: {schema['start_iso']} ~ {schema['end_iso']}\n\n"
                f"[ëŒ“ê¸€ ìƒ˜í”Œ]:\n{a_text}\n"
            )
            out = call_gemini_rotating(GEMINI_MODEL, GEMINI_API_KEYS, system_instruction, prompt_q,
                                       timeout_s=GEMINI_TIMEOUT, max_tokens=GEMINI_MAX_TOKENS)
            st.markdown(out)

            # ===== ì •ëŸ‰ í•˜ì´ë¼ì´íŠ¸ =====
            st.markdown("---")
            st.subheader("ğŸ“Š ì •ëŸ‰ í•˜ì´ë¼ì´íŠ¸")
            # ì‹œì ë³„ ì¶”ì´
            ts, label = timeseries_from_file(csv_path)
            if ts is not None and not ts.empty:
                fig_ts = px.line(ts, x="bucket", y="count", markers=True, title=f"{label} ëŒ“ê¸€ëŸ‰ ì¶”ì´ (KST)")
                st.plotly_chart(fig_ts, use_container_width=True)
            # ì¢‹ì•„ìš” Top10 (ê°„ë‹¨)
            best = []
            for chunk in pd.read_csv(csv_path, usecols=["video_id","video_title","author","text","likeCount"], chunksize=200_000):
                chunk["likeCount"] = pd.to_numeric(chunk["likeCount"], errors="coerce").fillna(0).astype(int)
                best.append(chunk.sort_values("likeCount", ascending=False).head(10))
            if best:
                df_top = pd.concat(best).sort_values("likeCount", ascending=False).head(10)
                st.markdown("#### ğŸ‘ ì¢‹ì•„ìš” Top10 ëŒ“ê¸€")
                for _, row in df_top.iterrows():
                    url = f"https://www.youtube.com/watch?v={row['video_id']}"
                    st.markdown(
                        f"<div style='margin-bottom:15px;'>"
                        f"<b>{int(row['likeCount'])} ğŸ‘</b> â€” {row.get('author','')}<br>"
                        f"<span style='font-size:14px;'>â–¶ï¸ <a href='{url}' target='_blank' style='color:black; text-decoration:none;'>"
                        f"{str(row.get('video_title','(ì œëª©ì—†ìŒ)'))[:60]}</a></span><br>"
                        f"> {str(row.get('text',''))[:150]}{'â€¦' if len(str(row.get('text','')))>150 else ''}"
                        f"</div>", unsafe_allow_html=True
                    )

            # ë‹¤ìš´ë¡œë“œ
            st.markdown("---")
            with open(csv_path, "rb") as f:
                st.download_button("â¬‡ï¸ ì „ì²´ ëŒ“ê¸€ CSV", data=f.read(), file_name=f"chatbot_full_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")

# ===================== í•˜ë‹¨ ë„êµ¬ =====================
st.markdown("---")
cols = st.columns(2)
with cols[0]:
    if st.button("ğŸ”„ ì´ˆê¸°í™”", type="secondary"):
        st.session_state.clear(); safe_rerun()
with cols[1]:
    if st.button("ğŸ§¹ ìºì‹œ/ë©”ëª¨ë¦¬ ì •ë¦¬"):
        st.cache_data.clear(); gc.collect(); st.success("ìºì‹œ/ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
